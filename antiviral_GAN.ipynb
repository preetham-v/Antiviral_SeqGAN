{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "antiviral_GAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5ZONMcvUfbNKJMd3Jy2+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetham-v/antiviralGAN/blob/master/antiviral_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvVxFsCD9mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQinJOXdHrU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary of 20 canonical amino acids\n",
        "CHARACTER_DICT = set([\n",
        "    u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K',\n",
        "    u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W',\n",
        "    u'V', u'Y']\n",
        ")\n",
        "\n",
        "CHARACTER_TO_INDEX = {\n",
        "    character: i\n",
        "    for i, character in enumerate(CHARACTER_DICT)\n",
        "}\n",
        "\n",
        "INDEX_TO_CHARACTER = {\n",
        "    CHARACTER_TO_INDEX[c]: c\n",
        "    for c in CHARACTER_TO_INDEX\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqm5jJ_NG1pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 18 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
        "num_amino_acids = len(CHARACTER_DICT) "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCHSqHfIOg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequence_to_vector(sequence, embed_dict=None):\n",
        "    if embed_dict==None:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(CHARACTER_TO_INDEX)])\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            default[i][CHARACTER_TO_INDEX[character]] = 1\n",
        "        return default\n",
        "    else:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])])\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            for k,val in enumerate(embed_dict[character]):\n",
        "                default[i,k] = val\n",
        "        return default"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtK8LnPeImWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "aed0a0cd-a101-4220-a3f9-9d9ec50c2186"
      },
      "source": [
        "sequence = 'AAFFAAYYYYT'\n",
        "encoding = sequence_to_vector(sequence)\n",
        "encoding = np.reshape(encoding, 360)\n",
        "print(encoding)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYPLrCaQnXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_length, output_activation=None):\n",
        "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
        "        Args:\n",
        "            input_length (int array): max_length * number_of_characters \n",
        "                                      (\"noise vector\")\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "            output_activation: torch activation function or None\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.linear1 = nn.Linear(360, 180)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.linear2 = nn.Linear(180, 360)\n",
        "        self.linear3 = nn.Linear(360, 360)\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        intermediate = self.linear1(input_tensor)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear2(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear3(intermediate)\n",
        "        if self.output_activation is not None:\n",
        "            intermediate = self.output_activation(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3i4Xoq9mqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, layers):\n",
        "        \"\"\"A discriminator for discerning real from generated samples.\n",
        "        params:\n",
        "            input_dim (int): width of the input\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "        Output activation is Sigmoid.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self._init_layers(layers)\n",
        "\n",
        "    def _init_layers(self, layers):\n",
        "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
        "        self.module_list = nn.ModuleList()\n",
        "        last_layer = self.input_dim\n",
        "        for index, width in enumerate(layers):\n",
        "            self.module_list.append(nn.Linear(last_layer, width))\n",
        "            last_layer = width\n",
        "            if index + 1 != len(layers):\n",
        "                self.module_list.append(nn.LeakyReLU())\n",
        "        else:\n",
        "            self.module_list.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
        "        intermediate = input_tensor\n",
        "        for layer in self.module_list:\n",
        "            intermediate = layer(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIaAh98vmoNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_function(batch_size):\n",
        "  input_array = []\n",
        "  a = np.zeros([18,20])\n",
        "  for j in range(batch_size):\n",
        "    for i in range(18):\n",
        "      if i < 3:\n",
        "        a[i][0] = 1\n",
        "      else:\n",
        "        x = np.random.randint(20)\n",
        "        a[i][x] = 1\n",
        "    input_array.append(np.reshape(a,360))\n",
        "\n",
        "  return input_array\n",
        "\n",
        "def noise_function(batch_size):\n",
        "  input_array = []  \n",
        "  a = np.zeros([18,20])\n",
        "  for j in range(batch_size):\n",
        "    for i in range(18):\n",
        "      x = np.random.randint(20)\n",
        "      a[i][x] = 1\n",
        "    input_array.append(np.reshape(a,360))\n",
        "  \n",
        "  return input_array"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphmNsOt-ikc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class VanillaGAN():\n",
        "    def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
        "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
        "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
        "        Args:\n",
        "            generator: a Ganerator network\n",
        "            discriminator: A Discriminator network\n",
        "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
        "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
        "            batch_size: training batch size\n",
        "            device: cpu or CUDA\n",
        "            lr_d: learning rate for the discriminator\n",
        "            lr_g: learning rate for the generator\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.generator = self.generator.to(device)\n",
        "        self.discriminator = discriminator\n",
        "        self.discriminator = self.discriminator.to(device)\n",
        "        self.noise_fn = noise_fn\n",
        "        self.data_fn = data_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optim_d = optim.Adam(discriminator.parameters(),\n",
        "                                  lr=lr_d, betas=(0.5, 0.999))\n",
        "        self.optim_g = optim.Adam(generator.parameters(),\n",
        "                                  lr=lr_g, betas=(0.5, 0.999))\n",
        "        self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
        "        self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "    def generate_samples(self, latent_vec=None, num=None):\n",
        "        \"\"\"Sample from the generator.\n",
        "        Args:\n",
        "            latent_vec: A pytorch latent vector or None\n",
        "            num: The number of samples to generate if latent_vec is None\n",
        "        If latent_vec and num are None then us self.batch_size random latent\n",
        "        vectors.\n",
        "        \"\"\"\n",
        "        num = self.batch_size if num is None else num\n",
        "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
        "        with torch.no_grad():\n",
        "            samples = self.generator(latent_vec)\n",
        "        return samples\n",
        "\n",
        "    def train_step_generator(self):\n",
        "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
        "        self.generator.zero_grad()\n",
        "\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        generated = self.generator(latent_vec)\n",
        "        classifications = self.discriminator(generated)\n",
        "        loss = self.criterion(classifications, self.target_ones)\n",
        "        loss.backward()\n",
        "        self.optim_g.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def train_step_discriminator(self):\n",
        "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
        "        self.discriminator.zero_grad()\n",
        "\n",
        "        # real samples\n",
        "        real_samples = self.data_fn(self.batch_size)\n",
        "        pred_real = self.discriminator(real_samples)\n",
        "        loss_real = self.criterion(pred_real, self.target_ones)\n",
        "\n",
        "        # generated samples\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            fake_samples = self.generator(latent_vec)\n",
        "        pred_fake = self.discriminator(fake_samples)\n",
        "        loss_fake = self.criterion(pred_fake, self.target_zeros)\n",
        "\n",
        "        # combine\n",
        "        loss = (loss_real + loss_fake) / 2\n",
        "        loss.backward()\n",
        "        self.optim_d.step()\n",
        "        return loss_real.item(), loss_fake.item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Train both networks and return the losses.\"\"\"\n",
        "        loss_d = self.train_step_discriminator()\n",
        "        loss_g = self.train_step_generator()\n",
        "        return loss_g, loss_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC7m2z-iXxGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simpleGAN(batch_size: int = 5, training_steps: int = 10000):\n",
        "\n",
        "  #Array to monitor losses\n",
        "  loss_g = []\n",
        "  loss_d = []\n",
        "  input_length = 360\n",
        "\n",
        "  # Models\n",
        "  generator = Generator(input_length)\n",
        "  discriminator = Discriminator(input_length, [64, 32, 1])  \n",
        "\n",
        "  # Optimizers\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), \n",
        "                                          lr=0.001)\n",
        "\n",
        "  # loss\n",
        "  loss = nn.BCELoss()\n",
        "\n",
        "  for i in range(training_steps):\n",
        "\n",
        "    # zero the gradients on each iteration\n",
        "    generator_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    # Create noisy input for generator\n",
        "    # Need float type instead of int\n",
        "    noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
        "    generated_data = generator(noise)  \n",
        "\n",
        "    # Generate examples of even real data\n",
        "    true_data = data_function(batch_size)\n",
        "    true_labels = torch.tensor(np.ones(batch_size)).float()\n",
        "    true_data = torch.tensor(true_data).float()\n",
        "\n",
        "    # Train the generator\n",
        "    # We invert the labels here and don't train the discriminator because we want the generator\n",
        "    # to make things the discriminator classifies as true.\n",
        "    generator_discriminator_out = discriminator(generated_data)\n",
        "    generator_loss = loss(generator_discriminator_out, true_labels)\n",
        "    generator_loss.backward()\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    # Train the discriminator on the true/generated data\n",
        "    discriminator_optimizer.zero_grad()\n",
        "    true_discriminator_out = discriminator(true_data)\n",
        "    true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
        "\n",
        "    # add .detach() here think about this\n",
        "    generator_discriminator_out = discriminator(generated_data.detach())\n",
        "    generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
        "    discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
        "    discriminator_loss.backward()\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "      generated_data = generated_data.detach().numpy()\n",
        "      generated_data[generated_data > 0.5] = 1\n",
        "      generated_data[generated_data <= 0.5] = 0           \n",
        "      sequence = np.reshape(generated_data[0], [18,20])\n",
        "      print(i, sequence)\n",
        "      print(generator_loss, discriminator_loss)    \n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jUxoWZeFFy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "def main():\n",
        "    from time import time\n",
        "    epochs = 600\n",
        "    batches = 10\n",
        "    generator = Generator(360)\n",
        "    discriminator = Discriminator(1, [64, 32, 1])\n",
        "    noise_fn = noise_function()\n",
        "    data_fn = data_function()\n",
        "    gan = VanillaGAN(generator, discriminator, noise_fn, data_fn, device='cpu')\n",
        "    loss_g, loss_d_real, loss_d_fake = [], [], []\n",
        "    start = time()\n",
        "    for epoch in range(epochs):\n",
        "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
        "        for batch in range(batches):\n",
        "            lg_, (ldr_, ldf_) = gan.train_step()\n",
        "            loss_g_running += lg_\n",
        "            loss_d_real_running += ldr_\n",
        "            loss_d_fake_running += ldf_\n",
        "        loss_g.append(loss_g_running / batches)\n",
        "        loss_d_real.append(loss_d_real_running / batches)\n",
        "        loss_d_fake.append(loss_d_fake_running / batches)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
        "              f\" G={loss_g[-1]:.3f},\"\n",
        "              f\" Dr={loss_d_real[-1]:.3f},\"\n",
        "              f\" Df={loss_d_fake[-1]:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5ReVWzaZeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65df4ada-2475-4fa2-83dc-622460e0d407"
      },
      "source": [
        "simpleGAN()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.7490, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6896, grad_fn=<DivBackward0>)\n",
            "1000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.6906, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6940, grad_fn=<DivBackward0>)\n",
            "2000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "tensor(0.6920, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6935, grad_fn=<DivBackward0>)\n",
            "3000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.6948, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6938, grad_fn=<DivBackward0>)\n",
            "4000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.]]\n",
            "tensor(0.7047, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6884, grad_fn=<DivBackward0>)\n",
            "5000 [[1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]\n",
            " [1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.]]\n",
            "tensor(0.4634, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.8380, grad_fn=<DivBackward0>)\n",
            "6000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "tensor(0.6823, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.7100, grad_fn=<DivBackward0>)\n",
            "7000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.6928, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6978, grad_fn=<DivBackward0>)\n",
            "8000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.7068, grad_fn=<DivBackward0>)\n",
            "9000 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor(0.6927, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.6934, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CO22bJB8Bp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d4547e83-a160-48b9-a271-064d727e2dac"
      },
      "source": [
        "a = np.array([1,1,1,1,1,1])\n",
        "print(a.shape)\n",
        "a = np.reshape(a, [3,2])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6,)\n",
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzhhoHK9-Dlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}