{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "antiviral_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yAb5ZeeIA4e0"
      ],
      "authorship_tag": "ABX9TyM+nevWYppAvlE5j28Dfgkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetham-v/antiviralGAN/blob/master/antiviral_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAb5ZeeIA4e0",
        "colab_type": "text"
      },
      "source": [
        "##SimpleGAN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvVxFsCD9mT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwA9cE0NHuh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/preetham-v/antiviralGAN/master/AVPdb_data.csv'\n",
        "\n",
        "data = pd.read_csv(url, skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPnBUMVMJEK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "720e239a-2934-4ed8-c4d9-8e397b36c85a"
      },
      "source": [
        "all_sequences = np.asarray(data['seq'])\n",
        "print(all_sequences[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PYVGSGLYRR' 'SMIENLEYM' 'ECRSTSYAGAVVNDL' 'STSYAGAVVNDL' 'YAGAVVNDL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQinJOXdHrU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary of 20 canonical amino acids\n",
        "CHARACTER_DICT = set([\n",
        "    u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K',\n",
        "    u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W',\n",
        "    u'V', u'Y']\n",
        ")\n",
        "\n",
        "CHARACTER_TO_INDEX = {\n",
        "    character: i\n",
        "    for i, character in enumerate(CHARACTER_DICT)\n",
        "}\n",
        "\n",
        "INDEX_TO_CHARACTER = {\n",
        "    CHARACTER_TO_INDEX[c]: c\n",
        "    for c in CHARACTER_TO_INDEX\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqm5jJ_NG1pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 18 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
        "num_amino_acids = len(CHARACTER_DICT) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCHSqHfIOg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequence_to_vector(sequence, embed_dict=None):\n",
        "    if embed_dict==None:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(CHARACTER_TO_INDEX)+1])\n",
        "        default[:,len(CHARACTER_TO_INDEX)] = 1\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            default[i][CHARACTER_TO_INDEX[character]] = 1\n",
        "            default[i][len(CHARACTER_TO_INDEX)] = 0\n",
        "        return default\n",
        "    else:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])])\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            for k,val in enumerate(embed_dict[character]):\n",
        "                default[i,k] = val\n",
        "        return default"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KFAejGUmdnQz",
        "colab": {}
      },
      "source": [
        "def vector_to_sequence(vector):\n",
        "    seq = ''\n",
        "\n",
        "    for i in range(18):\n",
        "        arg = np.argmax(vector[i])\n",
        "        if arg == 20:\n",
        "          seq += 'X'\n",
        "        else:\n",
        "          seq += INDEX_TO_CHARACTER[arg]\n",
        "    return seq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYPLrCaQnXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_length):\n",
        "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
        "        Args:\n",
        "            input_length (int array): max_length * number_of_characters \n",
        "                                      (\"noise vector\")\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "            output_activation: torch activation function or None\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_length, 40)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(40, 120)\n",
        "        self.linear3 = nn.Linear(120, 240)\n",
        "        self.linear4 = nn.Linear(240, 378)\n",
        "        self.output_activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        intermediate = self.linear1(input_tensor)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear2(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear3(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear4(intermediate)\n",
        "        intermediate = self.output_activation(intermediate)\n",
        "\n",
        "        view = intermediate.view(-1, 18)\n",
        "        (view == view.max(dim=1, keepdim=True)[0]).view_as(intermediate).int()\n",
        "        \n",
        "        return intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3i4Xoq9mqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, layers):\n",
        "        \"\"\"A discriminator for discerning real from generated samples.\n",
        "        params:\n",
        "            input_dim (int): width of the input\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "        Output activation is Sigmoid.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self._init_layers(layers)\n",
        "\n",
        "    def _init_layers(self, layers):\n",
        "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
        "        self.module_list = nn.ModuleList()\n",
        "        last_layer = self.input_dim\n",
        "        for index, width in enumerate(layers):\n",
        "            self.module_list.append(nn.Linear(last_layer, width))\n",
        "            last_layer = width\n",
        "            if index + 1 != len(layers):\n",
        "                self.module_list.append(nn.LeakyReLU())\n",
        "            else:\n",
        "                self.module_list.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
        "        intermediate = input_tensor\n",
        "        for layer in self.module_list:\n",
        "            intermediate = layer(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgni6k90HhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "19ef5423-da91-4e1f-e987-dfa640e69d9f"
      },
      "source": [
        "all_inputs = []\n",
        "for j in range(len(all_sequences)):\n",
        "  embedding = sequence_to_vector(all_sequences[j])\n",
        "  all_inputs.append(np.reshape(embedding,378))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2e9453131e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mall_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m378\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a9d860619a17>\u001b[0m in \u001b[0;36msequence_to_vector\u001b[0;34m(sequence, embed_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msequence_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membed_dict\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARACTER_TO_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHARACTER_TO_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CHARACTER_TO_INDEX' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIaAh98vmoNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_function(batch_size, iteration):\n",
        "  input_array = all_inputs[batch_size*iteration : batch_size*(iteration+1)]\n",
        "  return input_array\n",
        "\n",
        "def noise_function(batch_size):\n",
        "  input_array = np.random.randn(batch_size,1,20)\n",
        "    # a = np.zeros([18,21])\n",
        "    # for i in range(18):\n",
        "    #   x = np.random.randint(21)\n",
        "    #   a[i][x] = 1\n",
        "    # input_array.append(np.reshape(a,378))\n",
        "  \n",
        "  return input_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphmNsOt-ikc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class VanillaGAN():\n",
        "    def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
        "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
        "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
        "        Args:\n",
        "            generator: a Ganerator network\n",
        "            discriminator: A Discriminator network\n",
        "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
        "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
        "            batch_size: training batch size\n",
        "            device: cpu or CUDA\n",
        "            lr_d: learning rate for the discriminator\n",
        "            lr_g: learning rate for the generator\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.generator = self.generator.to(device)\n",
        "        self.discriminator = discriminator\n",
        "        self.discriminator = self.discriminator.to(device)\n",
        "        self.noise_fn = noise_fn\n",
        "        self.data_fn = data_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optim_d = optim.Adam(discriminator.parameters(),\n",
        "                                  lr=lr_d, betas=(0.5, 0.999))\n",
        "        self.optim_g = optim.Adam(generator.parameters(),\n",
        "                                  lr=lr_g, betas=(0.5, 0.999))\n",
        "        self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
        "        self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "    def generate_samples(self, latent_vec=None, num=None):\n",
        "        \"\"\"Sample from the generator.\n",
        "        Args:\n",
        "            latent_vec: A pytorch latent vector or None\n",
        "            num: The number of samples to generate if latent_vec is None\n",
        "        If latent_vec and num are None then us self.batch_size random latent\n",
        "        vectors.\n",
        "        \"\"\"\n",
        "        num = self.batch_size if num is None else num\n",
        "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
        "        with torch.no_grad():\n",
        "            samples = self.generator(latent_vec)\n",
        "        return samples\n",
        "\n",
        "    def train_step_generator(self):\n",
        "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
        "        self.generator.zero_grad()\n",
        "\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        generated = self.generator(latent_vec)\n",
        "        classifications = self.discriminator(generated)\n",
        "        loss = self.criterion(classifications, self.target_ones)\n",
        "        loss.backward()\n",
        "        self.optim_g.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def train_step_discriminator(self):\n",
        "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
        "        self.discriminator.zero_grad()\n",
        "\n",
        "        # real samples\n",
        "        real_samples = self.data_fn(self.batch_size)\n",
        "        pred_real = self.discriminator(real_samples)\n",
        "        loss_real = self.criterion(pred_real, self.target_ones)\n",
        "\n",
        "        # generated samples\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            fake_samples = self.generator(latent_vec)\n",
        "        pred_fake = self.discriminator(fake_samples)\n",
        "        loss_fake = self.criterion(pred_fake, self.target_zeros)\n",
        "\n",
        "        # combine\n",
        "        loss = (loss_real + loss_fake) / 2\n",
        "        loss.backward()\n",
        "        self.optim_d.step()\n",
        "        return loss_real.item(), loss_fake.item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Train both networks and return the losses.\"\"\"\n",
        "        loss_d = self.train_step_discriminator()\n",
        "        loss_g = self.train_step_generator()\n",
        "        return loss_g, loss_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC7m2z-iXxGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simpleGAN(generator_func, discriminator_func, \n",
        "              batch_size: int = 25, epochs: int = 5, \n",
        "              max_data: int = len(all_sequences), print_every: int = 10, ):\n",
        "\n",
        "  #Array to monitor losses\n",
        "  loss_g = []\n",
        "  loss_dreal = []\n",
        "  loss_dfake = []\n",
        "  outputs = []\n",
        "  input_length = 20\n",
        "\n",
        "  # Models\n",
        "  generator = generator_func\n",
        "  discriminator = discriminator_func\n",
        "\n",
        "  # Optimizers\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), \n",
        "                                          lr=0.001)\n",
        "\n",
        "  # loss\n",
        "  loss = nn.BCELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    for j in range(int(max_data/batch_size)):\n",
        "\n",
        "      # zero the gradients on each iteration\n",
        "      generator_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      # Create noisy input for generator\n",
        "      # Need float type instead of int\n",
        "      noise = noise_function(batch_size)\n",
        "      noise_data = torch.tensor(noise).float()\n",
        "      generated_data = generator(noise_data)  \n",
        "\n",
        "      # Generate examples of real data\n",
        "      true_data = data_function(batch_size, j)\n",
        "      true_labels = torch.tensor(np.ones(batch_size)).float()\n",
        "      true_data = torch.tensor(true_data).float()\n",
        "\n",
        "      # Train the generator\n",
        "      # We invert the labels here and don't train the discriminator because we want the generator\n",
        "      # to make things the discriminator classifies as true.\n",
        "      generator_discriminator_out = discriminator(generated_data)\n",
        "      generator_loss = loss(generator_discriminator_out, true_labels)\n",
        "      generator_loss.backward()\n",
        "      generator_optimizer.step()\n",
        "\n",
        "      # Train the discriminator on the true/generated data\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      true_discriminator_out = discriminator(true_data)\n",
        "      true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
        "\n",
        "      # add .detach() here think about this\n",
        "      generator_discriminator_out = discriminator(generated_data.detach())\n",
        "      generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
        "      discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
        "      discriminator_loss.backward()\n",
        "      discriminator_optimizer.step()\n",
        "\n",
        "      loss_g.append(generator_loss.detach().numpy())\n",
        "      loss_dreal.append(true_discriminator_loss.detach().numpy())\n",
        "      loss_dfake.append(generator_discriminator_loss.detach().numpy())\n",
        "  \n",
        "\n",
        "    \n",
        "    generated_data = generated_data.detach().numpy()\n",
        "#      x = random.randint(0,len(generated_data))\n",
        "    sequences = set()\n",
        "    \n",
        "    for j in range(batch_size):\n",
        "      sequence = np.reshape(generated_data[j], [18,21])\n",
        "      sequences.add(vector_to_sequence(sequence))\n",
        "\n",
        "    seq_update = list(sequences)\n",
        "    outputs.append(seq_update)\n",
        "\n",
        "    if i % print_every == 0:\n",
        "\n",
        "      print(\"Currently at epoch: \" + str(i))\n",
        "\n",
        "  return loss_g, loss_dreal, loss_dfake, outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jUxoWZeFFy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "def main():\n",
        "    from time import time\n",
        "    epochs = 600\n",
        "    batches = 10\n",
        "    generator = Generator(360)\n",
        "    discriminator = Discriminator(1, [64, 32, 1])\n",
        "    noise_fn = noise_function()\n",
        "    data_fn = data_function()\n",
        "    gan = VanillaGAN(generator, discriminator, noise_fn, data_fn, device='cpu')\n",
        "    loss_g, loss_d_real, loss_d_fake = [], [], []\n",
        "    start = time()\n",
        "    for epoch in range(epochs):\n",
        "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
        "        for batch in range(batches):\n",
        "            lg_, (ldr_, ldf_) = gan.train_step()\n",
        "            loss_g_running += lg_\n",
        "            loss_d_real_running += ldr_\n",
        "            loss_d_fake_running += ldf_\n",
        "        loss_g.append(loss_g_running / batches)\n",
        "        loss_d_real.append(loss_d_real_running / batches)\n",
        "        loss_d_fake.append(loss_d_fake_running / batches)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
        "              f\" G={loss_g[-1]:.3f},\"\n",
        "              f\" Dr={loss_d_real[-1]:.3f},\"\n",
        "              f\" Df={loss_d_fake[-1]:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5ReVWzaZeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bdee817-08e5-4357-ee1b-36c0cf57527e"
      },
      "source": [
        "import time\n",
        "\n",
        "torch.manual_seed(1104)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "generator = Generator(20)\n",
        "discriminator = Discriminator(378, [32, 16, 1])  \n",
        "\n",
        "losses = simpleGAN(generator, discriminator, \n",
        "                   batch_size=2059, epochs=40000, print_every = 500)\n",
        "\n",
        "outputs = losses[3]\n",
        "\n",
        "print(\"Program took\", time.time() - start_time, \"to run\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2059])) that is different to the input size (torch.Size([2059, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([2059])) that is different to the input size (torch.Size([2059, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Currently at epoch: 0\n",
            "Currently at epoch: 500\n",
            "Currently at epoch: 1000\n",
            "Currently at epoch: 1500\n",
            "Currently at epoch: 2000\n",
            "Currently at epoch: 2500\n",
            "Currently at epoch: 3000\n",
            "Currently at epoch: 3500\n",
            "Currently at epoch: 4000\n",
            "Currently at epoch: 4500\n",
            "Currently at epoch: 5000\n",
            "Currently at epoch: 5500\n",
            "Currently at epoch: 6000\n",
            "Currently at epoch: 6500\n",
            "Currently at epoch: 7000\n",
            "Currently at epoch: 7500\n",
            "Currently at epoch: 8000\n",
            "Currently at epoch: 8500\n",
            "Currently at epoch: 9000\n",
            "Currently at epoch: 9500\n",
            "Currently at epoch: 10000\n",
            "Currently at epoch: 10500\n",
            "Currently at epoch: 11000\n",
            "Currently at epoch: 11500\n",
            "Currently at epoch: 12000\n",
            "Currently at epoch: 12500\n",
            "Currently at epoch: 13000\n",
            "Currently at epoch: 13500\n",
            "Currently at epoch: 14000\n",
            "Currently at epoch: 14500\n",
            "Currently at epoch: 15000\n",
            "Currently at epoch: 15500\n",
            "Currently at epoch: 16000\n",
            "Currently at epoch: 16500\n",
            "Currently at epoch: 17000\n",
            "Currently at epoch: 17500\n",
            "Currently at epoch: 18000\n",
            "Currently at epoch: 18500\n",
            "Currently at epoch: 19000\n",
            "Currently at epoch: 19500\n",
            "Currently at epoch: 20000\n",
            "Currently at epoch: 20500\n",
            "Currently at epoch: 21000\n",
            "Currently at epoch: 21500\n",
            "Currently at epoch: 22000\n",
            "Currently at epoch: 22500\n",
            "Currently at epoch: 23000\n",
            "Currently at epoch: 23500\n",
            "Currently at epoch: 24000\n",
            "Currently at epoch: 24500\n",
            "Currently at epoch: 25000\n",
            "Currently at epoch: 25500\n",
            "Currently at epoch: 26000\n",
            "Currently at epoch: 26500\n",
            "Currently at epoch: 27000\n",
            "Currently at epoch: 27500\n",
            "Currently at epoch: 28000\n",
            "Currently at epoch: 28500\n",
            "Currently at epoch: 29000\n",
            "Currently at epoch: 29500\n",
            "Currently at epoch: 30000\n",
            "Currently at epoch: 30500\n",
            "Currently at epoch: 31000\n",
            "Currently at epoch: 31500\n",
            "Currently at epoch: 32000\n",
            "Currently at epoch: 32500\n",
            "Currently at epoch: 33000\n",
            "Currently at epoch: 33500\n",
            "Currently at epoch: 34000\n",
            "Currently at epoch: 34500\n",
            "Currently at epoch: 35000\n",
            "Currently at epoch: 35500\n",
            "Currently at epoch: 36000\n",
            "Currently at epoch: 36500\n",
            "Currently at epoch: 37000\n",
            "Currently at epoch: 37500\n",
            "Currently at epoch: 38000\n",
            "Currently at epoch: 38500\n",
            "Currently at epoch: 39000\n",
            "Currently at epoch: 39500\n",
            "Program took 9335.723873376846 to run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drm_MN96pPUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "433b5667-0ac0-4733-f22f-23ac946d630b"
      },
      "source": [
        "loss_g = losses[0]\n",
        "loss_dreal = losses[1]\n",
        "loss_dfake = losses[2]\n",
        "#plt.plot(loss_g, label = 'generator')\n",
        "#plt.plot(loss_dreal, label ='discriminator_Real')\n",
        "plt.plot(loss_dfake, label = 'discrimininator_Fake')\n",
        "plt.xlabel('Update')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff85fe71d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEjvvIz6kBNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "389c09c9-5b6a-4c19-be88-848033f67a7d"
      },
      "source": [
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c5b7e396cfc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlsIG8LP8rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('outputs.txt', 'w') as f:\n",
        "    for item in outputs:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mmh3YVl-dUdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f770d220-dbc5-46c1-dbb5-45f9b4abdd68"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('outputs.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_eef95cb7-9641-437f-9485-af1408629683\", \"outputs.txt\", 7089768)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7BrMAa4zcC7",
        "colab_type": "text"
      },
      "source": [
        "##NEW CODE - SeqGAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWs2msGddLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All dependencies \n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pdb\n",
        "import math\n",
        "import torch.nn.init as init\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grt5UNBW6Chv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download and extract Antiviral Sequences from AVPdb\n",
        "\n",
        "MAX_SEQ_LEN = 18 #2000 kDa / 110 kDa = 18\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/preetham-v/antiviralGAN/master/AVPdb_data.csv'\n",
        "\n",
        "data = pd.read_csv(url, skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])\n",
        "\n",
        "all_sequences = np.asarray(data['seq'])\n",
        "\n",
        "#Two way dictionary of 20 canonical amino acids (not efficient, but helpful!)\n",
        "CHARACTER_DICT = {\n",
        "    'A': 1, 'C': 2, 'E': 3, 'D': 4, 'F': 5, 'I': 6, 'H': 7, \n",
        "    'K': 8, 'M': 9, 'L': 10, 'N': 11, 'Q': 12, 'P': 13, 'S': 14, \n",
        "    'R': 15, 'T': 16, 'W': 17, 'V': 18, 'Y': 19, 'G': 20}\n",
        "\n",
        "INDEX_DICT = {\n",
        "    1: 'A', 2: 'C', 3: 'E', 4: 'D', 5: 'F', 6: 'I', 7: 'H',\n",
        "    8: 'K', 9: 'M', 10: 'L', 11: 'N', 12: 'Q', 13: 'P', 14: 'S',\n",
        "    15: 'R', 16: 'T', 17: 'W', 18: 'V', 19: 'Y', 20: 'G', 21: 'X'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK-ITgncO-SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "cd0db331-67a0-49cc-aa3c-aab4ac068dc6"
      },
      "source": [
        "#Checks for the distribution of sequences in Dataset\n",
        "\n",
        "count = np.zeros(100)\n",
        "\n",
        "for seq in all_sequences:\n",
        "  count[len(seq)] += 1\n",
        "\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Number of residues')\n",
        "plt.ylabel('Number of sequences')\n",
        "plt.bar(np.arange(100), count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 100 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe9ElEQVR4nO3debgcVbnv8e+PSQaBAIkRkkAAUQ84BIyAooh4VAYP4XAF4aogIoF7iaKi14AiOKBxAJULB2WSoAhykSEaZJDDIHoZkoDMHDAGCIQkCJgAMoS854+1dlHZ7KH2Tlf33t2/z/Psp7tWVa96qyvpt2rVqlWKCMzMzABWaXUAZmY2dDgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzFpA0i6S5je4zmMkndnA+p6RtEV+f46kbzew7p9KOrZR9VnjOCl0KEnvkfRnSf+Q9KSkP0l6Z6vjagZJIekNw2mdkq6T9LykpZKWSJotaaqk13QtExHfiYjPVKyr3+Ui4rURMXewMZfW9ylJN3ar+/CI+NbK1m2N56TQgSStB/wO+L/AhsAY4BvAC62My/o1JSLWBTYGjgL2By6XpEauRNJqjazPhhcnhc70RoCIOD8iXo6If0bEVRFxR9cCkj4t6V5JT0m6UtJmpXkflHRfPss4RdL1XUeeko6X9MvSsuPzUfJqeXp9SWdJWiDpUUnflrRqnvcpSTdK+mFe798k7V6qa0NJP5f0WJ5/aWneRyTdLunpfAb0toF+KZJek9f9sKSFuYljrTxvF0nzJR0laVGO/+DSZzeS9Nt8FH9r3q4b87wb8mJ/yU0yHyt9rsf6+hIRz0bEdcBewLuAPXNdxXcvaU1Jv5T09/yd3CpptKQTgPcCp+RYTsnLh6QjJD0APFAqK5/djJR0dT5bub7r30T3fZzLrpP0GUn/AvwUeFde39N5/grNUZIOlfRgPmudIWmT0ryQdLikB/K2nNroRGivcFLoTP8FvCxpuqTdJW1QnilpEnAMsA8wCvgjcH6eNxK4GPgaMBL4K7DTANZ9DrAMeAOwLfAhoNyUsQNwf677+8BZpR+AXwBrA9sArwN+lGPaFjgbOAzYCPgZMKPctFLRNFLCnJDjGwN8vTT/9cD6ufwQ4NTSd3cq8Gxe5qD8B0BE7Jzfvj03yfy6Qn39ioiHgVmkH/nuDsp1jyN9J4cD/4yIr5L255Qcy5TSZ/Ymff9b97LKjwPfIu2b24HzKsR4b173/8/rG9F9GUm7At8F9iOdBT0EXNBtsY8A7wTelpf7cH/rtsFxUuhAEbEEeA8QwBnA4nx0Njovcjjw3Yi4NyKWAd8BJuQjwz2AuyPiooh4Cfgx8HiV9eb69wA+n492F5F+2PcvLfZQRJwRES8D00k/EqMlbQzsDhweEU9FxEsRcX3+zGTgZxFxcz7zmU5qCtux6neSE89k4AsR8WRELM3bXY7tJeCbed2XA88Ab8pnOv8DOC4inouIe3Ls/emxvqoxZ4+RmgB7qnsj4A35O5md93tfvpu3/Z+9zJ8ZETdExAvAV0lH/+MGGG9PPg6cHRFzct1H57rHl5aZFhFP50R4LSlxWw2cFDpU/sH/VESMBd4CbEL6gQfYDPhJPlV/GngSEOmIdhPgkVI9UZ7ux2bA6sCCUt0/Ix31dykSTEQ8l9++lnTE+2REPNVLvUd11ZnrHZdjrWoU6SxkdqmOK3J5l7/nJNnluRzbKGA1VvweqnwnvdU3EGNI+6e7XwBXAhfk5rbvS1q9n7r6i7m835/J6x3Id9ybTUhnB+W6/07ati7lA4/BfE9WkZOCERH3kZp13pKLHgEOi4gRpb+1IuLPwALSDy5QHGGXjxafJf24dnl96f0jpCP4kaV614uIbSqE+QiwoaRXNT/keSd0i3ftiDi/Qr1dngD+CWxTqmP9iKjy47OY1CQ2tlTWiCPoPuWj9HeQmoNWkM8+vhERWwPvJjW/HNg1u5cq+xsyubzfX0s6Q3mMtM+h9/3eX72PkRJ7V93rkM5yHu3nc1YDJ4UOJOnN+QLn2Dw9DjgAuCkv8lPgaEnb5PnrS9o3z5sJbCNpn3xh8XOs+ANwO7CzpE0lrU9qCgAgIhYAVwEnSlpP0iqStpT0vv5izp/9PfAfkjaQtLqkrrb6M4DDJe2gZB1Je0pat48q18gXY9eUtCbpTOgM4EeSXpe3e4ykftuuc1PXxcDxktaW9GZe+QHushDYor+6qsjreB9wGXALcHkPy7xf0ltz09YSUnPS8pWMZQ+lrsxrkK4t3BQRj0TEYtIP+CckrSrp08CWpc8tBMbmz/XkfOBgSRPydaDvADdHxLxBxGgryUmhMy0lXVC8WdKzpGRwF6mbIxFxCfA9UtPDkjxv9zzvCWBf0kXZvwNbAX/qqjgirgZ+DdwBzCZ1fS07EFgDuAd4CriIdN2gik+SftzuAxYBn8/rnAUcCpyS63wQ+FQ/dd1NOjPo+jsY+Er+7E15u/9A9Tb+KaQLu4+Tmm7OZ8UuvscD03PT1H4V6+zuFElLST+yPwZ+A+wWEct7WPb1pO92CXAvcH2OC+AnwEeVenCdPID1/wo4jtRs9A7gE6V5hwJfJv2b2Ab4c2nef5K+78clPdG90oj4A3Bs3p4FpISyf/flrDnkh+zYypJ0HfDLiGjY3bTDnaTvAa+PiIP6XdhsCPGZglkD5Ca5t+Xmq+1JXUwvaXVcZgPlOxfNGmNdUpPRJqTmnRNJbf5mw4qbj8zMrODmIzMzKwzr5qORI0fG+PHjWx2GmdmwMnv27CciYlRP84Z1Uhg/fjyzZs1qdRhmZsOKpId6m1db85GkcZKulXSPpLslHZnLj1caHfP2/LdH6TNHK42UeH+Vm4bMzKyx6jxTWAYcFRFz8p2lsyVdnef9KCJ+WF5Y0takG1a2IfXg+IOkN+a7Rc3MrAlqO1OIiAURMSe/X0q6q3JMHx+ZBFwQES9ExN9Id5ZuX1d8Zmb2ak3pfZSHwN0WuDkXTZF0h6SzS+PHj2HFURrn03cSMTOzBqs9KeTRFH9DGkN/CXAaaWyTCaRxTk4cYH2TJc2SNGvx4sUNj9fMrJPVmhTy+O2/Ac6LiIsBImJhfujHctKolF1NRI+y4nDDY+lh6NyIOD0iJkbExFGjeuxRZWZmg1Rn7yMBZwH3RsRJpfLyiJj/ThqBE2AGsL/Sc3I3J42+eUtd8ZmZ2avV2ftoJ9JQx3dKuj2XHQMcIGkC6cEb80jP1SUi7pZ0IWlI5WXAEe55ZGbWXLUlhYi4kfTgku5e9UCQ0mdOAE6oKyYzM+ubxz5qkvFTZzJ+6sxWh2Fm1icnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCnWOkmo1KI+fNG/ani2MxMzakc8UzMys4KRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKtSUFSeMkXSvpHkl3Szoyl28o6WpJD+TXDXK5JJ0s6UFJd0jarq7YzMysZ3WeKSwDjoqIrYEdgSMkbQ1MBa6JiK2Aa/I0wO7AVvlvMnBajbGZmVkPaksKEbEgIubk90uBe4ExwCRgel5sOrB3fj8JODeSm4ARkjauKz4zM3u1plxTkDQe2Ba4GRgdEQvyrMeB0fn9GOCR0sfm57LudU2WNEvSrMWLF9cWs5lZJ6o9KUh6LfAb4PMRsaQ8LyICiIHUFxGnR8TEiJg4atSoBkZqZma1JgVJq5MSwnkRcXEuXtjVLJRfF+XyR4FxpY+PzWVmZtYkdfY+EnAWcG9EnFSaNQM4KL8/CLisVH5g7oW0I/CPUjOTmZk1wWo11r0T8EngTkm357JjgGnAhZIOAR4C9svzLgf2AB4EngMOrjE2MzPrQW1JISJuBNTL7A/0sHwAR9QVj5mZ9c93NJuZWcFJwczMCk4KZmZW6DcpSDpS0nq5V9BZkuZI+lAzgjMzs+aqcqbw6XzT2YeADUg9iqbVGpWZmbVElaTQ1YNoD+AXEXE3vfcqMjOzYaxKUpgt6SpSUrhS0rrA8nrDMjOzVqhyn8IhwARgbkQ8J2kjfGOZmVlbqnKmEMDWwOfy9DrAmrVFZGZmLVMlKfwH8C7ggDy9FDi1tojMzKxlqjQf7RAR20m6DSAinpK0Rs1xmZlZC1Q5U3hJ0qrk5x5IGoUvNJuZtaUqSeFk4BLgdZJOAG4EvlNrVGZm1hL9Nh9FxHmSZpNGNhWwd0TcW3tkZtn4qTOL9/Om7dnCSMzaX79JIT/w5u6IODVPrydph4i4ufbozMysqao0H50GPFOafiaXmZlZm6k0zEV+AA4AEbGcep/YZmZmLVIlKcyV9DlJq+e/I4G5dQdmZmbNVyUpHA68G3gUmA/sAEyuMygzM2uNKr2PFgH7NyEWMzNrsSq9j0YBhwLjy8tHxKfrC8vcDdPMWqHKBePLgD8CfwBerjccMzNrpSpJYe2I+ErtkZiZWctVudD8O0l71B6JmZm1XJWkcCQpMTwvaYmkpZKW1B2YmZk1X5XeR+s2IxAzM2u9fs8UlHxC0rF5epyk7esPzczMmm0gT177n3n6GfzkNTOztuQnr5mZWcFPXjMzs4KfvGZmZgU/ec3MzApVxj7aFHgO+G25LCIerjMwMzNrvirNRzOB3+XXa0jPUvh9fx+SdLakRZLuKpUdL+lRSbfnvz1K846W9KCk+yV9eOCbYmZmK6tK89Fby9OStgP+d4W6zwFOAc7tVv6jiPhhtzq3Jg3PvQ2wCfAHSW+MCA/AZ2bWRFXOFFYQEXNID9rpb7kbgCcrVjsJuCAiXoiIvwEPAr5BzsysyapcU/hiaXIVYDvgsZVY5xRJBwKzgKMi4ilgDHBTaZn5uayneCaTn/y26aabrkQYZmbWXZUzhXVLf68hXVuYNMj1nQZsCUwAFgAnDrSCiDg9IiZGxMRRo0YNMgwzM+tJlWsK32jUyiJiYdd7SWeQLmBDev7zuNKiY3OZmZk1UZXmo9+S72buSUTsVXVlkjaOiAV58t+Brp5JM4BfSTqJdKF5K+CWqvWamVljVBn7aC7weuCXefoAYCFwaV8fknQ+sAswUtJ84DhgF0kTSElmHnAYQETcLelC4B5gGXCEex6ZmTVflaSwU0RMLE3/VtKsiPhCXx+KiAN6KD6rj+VPAE6oEI+ZmdWkyoXmdSRt0TUhaXNgnfpCMjOzVqlypvAF4DpJc0ljH21GbvYxM7P2UqX30RWStgLenIvui4gX6g3LzMxaocrjONcGvgxMiYi/AJtK+kjtkZmZWdNVuabwc+BF0iM5Id0/8O3aIjIzs5apck1hy4j4mKQDACLiOUmqOS5bCeOnzizez5u2ZwsjMbPhpkpSeFHSWrzyOM4tAV9TGIacLMysP1WSwnHAFcA4SecBOwGfqjMoMzNrjSq9j66WNAfYkdQl9ciIeKL2yMzMrOmq9D7aCXg+ImYCI4BjJG1We2RmZtZ0VXofnQY8J+ntwBeBv/Lqp6mZmVkbqJIUlkVEkJ6hcGpEnEp6toKZmbWZKheal0o6GvgEsLOkVYDV6w3LzMxaocqZwsdIXVAPiYjHSQ/A+UGtUZmZWUtU6X30OHBSafphfE3BzKwtVTlTMDOzDuGkYGZmhV6TgqRr8uv3mheOmZm1Ul/XFDaW9G5gL0kXkO5mLkTEnFojMzOzpusrKXwdOJbU2+ikbvMC2LWuoMzMrDV6TQoRcRFwkaRjI+JbTYzJzMxapEqX1G9J2gvYORddFxG/qzcsMzNrhSoD4n0XOBK4J/8dKek7dQdmZmbNV2WYiz2BCRGxHEDSdOA24Jg6AzMzs+arep/CiNL79esIxMzMWq/KmcJ3gdskXUvqlrozMLXWqMzMrCWqXGg+X9J1wDtz0VfyeEhmZtZmqpwpEBELgBk1x2JmZi3msY/MzKzgpGBmZoU+k4KkVSXd16xgzMystfpMChHxMnC/pE2bFI+ZmbVQlQvNGwB3S7oFeLarMCL2qi0qMzNriSpJ4djaozAzsyGh3wvNEXE9MA9YPb+/Fej3WQqSzpa0SNJdpbINJV0t6YH8ukEul6STJT0o6Q5J2w16i8zMbNCqDIh3KHAR8LNcNAa4tELd5wC7dSubClwTEVsB1/DKndG7A1vlv8nAaRXqNzOzBqvSJfUIYCdgCUBEPAC8rr8PRcQNwJPdiicB0/P76cDepfJzI7kJGCFp4wqxmZlZA1VJCi9ExItdE5JWIz15bTBG57ujAR4HRuf3Y4BHSsvNz2WvImmypFmSZi1evHiQYZiZWU+qJIXrJR0DrCXpg8D/A367siuOiGAQySUiTo+IiRExcdSoUSsbhpmZlVRJClOBxcCdwGHA5cDXBrm+hV3NQvl1US5/FBhXWm5sLjMzsyaqMkrq8vxgnZtJR/b356P8wZgBHARMy6+XlcqnSLoA2AH4R6mZyczMmqTfpCBpT+CnwF9Jz1PYXNJhEfH7fj53PrALMFLSfOA4UjK4UNIhwEPAfnnxy4E9gAeB54CDB7U1Zma2UqrcvHYi8P6IeBBA0pbATKDPpBARB/Qy6wM9LBukXk5mZtZCVa4pLO1KCNlcYGlN8ZiZWQv1eqYgaZ/8dpaky4ELSdcU9iXd1WxmZm2mr+ajfyu9Xwi8L79fDKxVW0RmZtYyvSaFiPDFXjOzDlOl99HmwGeB8eXlPXS2mVn7qdL76FLgLNJdzMvrDcfMzFqpSlJ4PiJOrj0SMzNruSpJ4SeSjgOuAl7oKoyIfp+pYGZmw0uVpPBW4JPArrzSfBR52hpo/NSZAMybtmeLIzGzTlUlKewLbFEePtvMzNpTlTua7wJG1B2ImZm1XpUzhRHAfZJuZcVrCu6SambWZqokheNqj8LMzIaEKs9TuL4ZgZiZWetVuaN5Ka88NnMNYHXg2YhYr87AzMys+aqcKazb9V6SgEnAjnUGZWZmrVGl91EhkkuBD9cUj5mZtVCV5qN9SpOrABOB52uLyMzMWqZK76PycxWWAfNITUhmZtZmqlxT8HMVWqxr+Aszs7r19TjOr/fxuYiIb9UQj5mZtVBfZwrP9lC2DnAIsBHgpGBm1mb6ehzniV3vJa0LHAkcDFwAnNjb58zMbPjq85qCpA2BLwIfB6YD20XEU80IzMzMmq+vawo/APYBTgfeGhHPNC0qMzNrib5uXjsK2AT4GvCYpCX5b6mkJc0Jz8zMmqmvawoDutvZzMyGP//wm5lZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFaqMkmrDgAfNM7NGaElSkDQPWAq8DCyLiIn57ulfA+NJw3PvN9zvnu76oZ43bc8WR2JmVk0rm4/eHxETImJinp4KXBMRWwHX5GkzM2uioXRNYRJpfCXy694tjMXMrCO1KikEcJWk2ZIm57LREbEgv38cGN3TByVNljRL0qzFixc3I1Yzs47RqgvN74mIRyW9Drha0n3lmRERkqKnD0bE6aRB+pg4cWKPy5iZ2eC05EwhIh7Nr4uAS4DtgYWSNgbIr4taEZuZWSdr+pmCpHWAVSJiaX7/IeCbwAzgIGBafr2s2bG1I3dVNbOBaEXz0WjgEkld6/9VRFwh6VbgQkmHAA8B+7UgNjOzjtb0pBARc4G391D+d+ADzY6nFXz/gpkNVb6j2YYkN3uZtcZQuk/BOtz4qTOdDMxazEnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRglQ20d5B7E5kNP04KZmZWcFKwhvLZgdnw5qRgZmYFJwUzMys4KTSAm0zMrF04KbSYE4qZDSUeJXUYG07JpFHDhZe32UOPmzWezxTMzKzgpDBIvTX7uDnIzIYzJwV7FSc2s87lawodym3zZtYTnymYmVnBScEANxmZWeKkYGZmBScFMzMrOCmYmVnBvY9s2HIPKrPG85mCDUo7XJhuh20wazQnBTMzKzgpWJ+qHE2vzBG3j9bNhhZfU7C242sNZoPnMwUzMyv4TGEAGvVMAGsenzWYDYyTgnUMJwiz/rn5yMzMCj5T6IGbieo1FL/f3mIql1dZZijwGZGtjCGXFCTtBvwEWBU4MyKmtTKeofYf3nrWzt1a/SNvzTSkkoKkVYFTgQ8C84FbJc2IiHtaG5l1kioHAu1w1jDUYl0ZTpyNM6SSArA98GBEzAWQdAEwCXBSsGFnuCQOGHjz2VDchuFuqCQ2RUTLVt6dpI8Cu0XEZ/L0J4EdImJKaZnJwOQ8+Sbg/pVc7UjgiZWsY7jxNncGb3NnGMw2bxYRo3qaMdTOFPoVEacDpzeqPkmzImJio+obDrzNncHb3Bkavc1DrUvqo8C40vTYXGZmZk0w1JLCrcBWkjaXtAawPzCjxTGZmXWMIdV8FBHLJE0BriR1ST07Iu6uebUNa4oaRrzNncHb3Bkaus1D6kKzmZm11lBrPjIzsxZyUjAzs0LHJgVJu0m6X9KDkqa2Op46SBon6VpJ90i6W9KRuXxDSVdLeiC/btDqWBtN0qqSbpP0uzy9uaSb8/7+de7I0DYkjZB0kaT7JN0r6V3tvp8lfSH/u75L0vmS1my3/SzpbEmLJN1VKutxvyo5OW/7HZK2G8w6OzIplIbT2B3YGjhA0tatjaoWy4CjImJrYEfgiLydU4FrImIr4Jo83W6OBO4tTX8P+FFEvAF4CjikJVHV5yfAFRHxZuDtpG1v2/0saQzwOWBiRLyF1DFlf9pvP58D7NatrLf9ujuwVf6bDJw2mBV2ZFKgNJxGRLwIdA2n0VYiYkFEzMnvl5J+KMaQtnV6Xmw6sHdrIqyHpLHAnsCZeVrArsBFeZG22mZJ6wM7A2cBRMSLEfE0bb6fSb0n15K0GrA2sIA2288RcQPwZLfi3vbrJODcSG4CRkjaeKDr7NSkMAZ4pDQ9P5e1LUnjgW2Bm4HREbEgz3ocGN2isOryY+D/AMvz9EbA0xGxLE+32/7eHFgM/Dw3mZ0paR3aeD9HxKPAD4GHScngH8Bs2ns/d+ltvzbkd61Tk0JHkfRa4DfA5yNiSXlepD7JbdMvWdJHgEURMbvVsTTRasB2wGkRsS3wLN2aitpwP29AOjLeHNgEWIdXN7O0vTr2a6cmhY4ZTkPS6qSEcF5EXJyLF3adVubXRa2KrwY7AXtJmkdqFtyV1N4+IjczQPvt7/nA/Ii4OU9fREoS7byf/xX4W0QsjoiXgItJ+76d93OX3vZrQ37XOjUpdMRwGrkt/Szg3og4qTRrBnBQfn8QcFmzY6tLRBwdEWMjYjxpv/5nRHwcuBb4aF6s3bb5ceARSW/KRR8gDTfftvuZ1Gy0o6S187/zrm1u2/1c0tt+nQEcmHsh7Qj8o9TMVFnH3tEsaQ9S23PXcBontDikhpP0HuCPwJ280r5+DOm6woXApsBDwH4R0f1i1rAnaRfgSxHxEUlbkM4cNgRuAz4RES+0Mr5GkjSBdGF9DWAucDDpoK9t97OkbwAfI/Wyuw34DKkNvW32s6TzgV1Iw2MvBI4DLqWH/ZqT4ymkZrTngIMjYtaA19mpScHMzF6tU5uPzMysB04KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYEOWpJB0Ymn6S5KOb1Dd50j6aP9LrvR69s2jll7b4HovlzSih/LjJX2pkeuyzuKkYEPZC8A+kka2OpCy0h2zVRwCHBoR729QfQBExB550DuzhnJSsKFsGen5s1/oPqP7kb6kZ/LrLpKul3SZpLmSpkn6uKRbJN0pactSNf8qaZak/8pjJnU9h+EHkm7NY9IfVqr3j5JmkO6c7R7PAbn+uyR9L5d9HXgPcJakH3RbfoX6+ljvxpJukHR7rvu9uXxeV7KU9NW8DTcCbyqt4zpJE/P7kXnoj762scd1WWcZ8BGKWZOdCtwh6fsD+MzbgX8hDTk8FzgzIrZXesjQZ4HP5+XGk4ZR3xK4VtIbgANJwwO8U9JrgD9Juiovvx3wloj4W3llkjYhjeP/DtIY/ldJ2jsivilpV9Jd1T3dWVrUJ2lyL+vdB7gyIk5Qeg7I2t3W/Q7ScB4TSP+f55BGC+3LIYNZl3UGJwUb0iJiiaRzSQ9U+WfFj93aNeaLpL8CXT/qdwLlZpwLI2I58ICkucCbgQ8BbyudhaxPemjJi8At3RNC9k7guohYnNd5Hun5Bpf2E2e5vt7WeytwttLAhpdGxO3d6ngvcElEPJfXXWUMr8GuyzqAk4INBz8mHQH/vFS2jNz8KWkV0pg/Xcpj3SwvTS9nxX/z3cd4CUDAZyPiyvKMPI7Ss4MLv1fl+npcb173zqSHBp0j6aSIOLdi/cV3BKxZ87qsTfiagg15eRC3C1nx0YrzSM01AHsBqw+i6n0lrZKvM2wB3A9cCfyvfLSMpDcqPbCmL7cA78vt9qsCBwDXDzCWHtcraTNgYUScQRrwrvtzd28A9pa0lqR1gX8rzZvHK99RuafVYNdlHcBnCjZcnAhMKU2fAVwm6S/AFQzuKP5h0g/6esDhEfG8pDNJ1xrmSBLpiWZ9PtIxIhZImkoatlnAzIgY6JDNva13F+DLkl4CniFd8yive46kXwN/IY2rf2tp9g+BC/P1ipkruy7rDB4l1czMCm4+MjOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwK/w3pmC8T22oLsAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EptLKy7U6V3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenises Sequence using dictionary above\n",
        "def sequence_to_vector(sequence):\n",
        "\n",
        "    default = np.asarray([21]*(MAX_SEQ_LEN))\n",
        "    for i, character in enumerate(sequence[:MAX_SEQ_LEN]):\n",
        "        default[i] = CHARACTER_DICT[character]\n",
        "    return default.astype(int)\n",
        "\n",
        "# Converts token vector to readable sequence\n",
        "# Only token outside of INDEX_DICT possible is 0,\n",
        "# and thus it will show up as 0 in sequence making it invalid\n",
        "\n",
        "def vector_to_sequence(vector):\n",
        "  \n",
        "    return ''.join([INDEX_DICT.get(item, '0')  for item in vector])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuUZiwOlAxIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bbba937f-ee67-42cf-cdef-f5939968491a"
      },
      "source": [
        "all_sequences = np.asarray(data['seq'])\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for i in range(len(all_sequences)):\n",
        "\n",
        "  all_data.append(sequence_to_vector(all_sequences[i]))\n",
        "\n",
        "#Sanity check whether the data has been featurized properly\n",
        "some_random_number = np.random.randint(len(all_sequences))\n",
        "print(all_sequences[some_random_number])\n",
        "print(all_data[some_random_number])\n",
        "print(vector_to_sequence(all_data[some_random_number]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RRKKAVALLPAVLLALLAP\n",
            "[15 15  8  8  1 18  1 10 10 13  1 18 10 10  1 10 10  1]\n",
            "RRKKAVALLPAVLLALLA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvGgl0eizU6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, gpu=False, oracle_init=False):\n",
        "        super(Generator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.gru2out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # initialise oracle network with N(0,1)\n",
        "        # otherwise variance of initialisation is very small => high NLL for data sampled from the same model\n",
        "        if oracle_init:\n",
        "            for p in self.parameters():\n",
        "                init.normal(p, 0, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        h = autograd.Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
        "\n",
        "        if self.gpu:\n",
        "            return h.cuda()\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        \"\"\"\n",
        "        Embeds input and applies GRU one token at a time (seq_len = 1)\n",
        "        \"\"\"\n",
        "        # input dim                                             # batch_size\n",
        "        emb = self.embeddings(inp)                              # batch_size x embedding_dim\n",
        "        emb = emb.view(1, -1, self.embedding_dim)               # 1 x batch_size x embedding_dim\n",
        "        out, hidden = self.gru(emb, hidden)                     # 1 x batch_size x hidden_dim (out)\n",
        "        out = self.gru2out(out.view(-1, self.hidden_dim))       # batch_size x vocab_size\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "        return out, hidden\n",
        "\n",
        "    def sample(self, num_samples, start_letter=0):\n",
        "        \"\"\"\n",
        "        Samples the network and returns num_samples samples of length max_seq_len.\n",
        "        Outputs: samples, hidden\n",
        "            - samples: num_samples x max_seq_length (a sampled sequence in each row)\n",
        "        \"\"\"\n",
        "\n",
        "        samples = torch.zeros(num_samples, self.max_seq_len).type(torch.LongTensor)\n",
        "\n",
        "        h = self.init_hidden(num_samples)\n",
        "        inp = autograd.Variable(torch.LongTensor([start_letter]*num_samples))\n",
        "\n",
        "        if self.gpu:\n",
        "            samples = samples.cuda()\n",
        "            inp = inp.cuda()\n",
        "\n",
        "        for i in range(self.max_seq_len):\n",
        "            out, h = self.forward(inp, h)               # out: num_samples x vocab_size\n",
        "            out = torch.multinomial(torch.exp(out), 1)  # num_samples x 1 (sampling from each row)\n",
        "            samples[:, i] = out.view(-1).data\n",
        "\n",
        "            inp = out.view(-1)\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def batchNLLLoss(self, inp, target):\n",
        "        \"\"\"\n",
        "        Returns the NLL Loss for predicting target sequence.\n",
        "        Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size x seq_len\n",
        "            inp should be target with <s> (start letter) prepended\n",
        "        \"\"\"\n",
        "\n",
        "        loss_fn = nn.NLLLoss()\n",
        "        batch_size, seq_len = inp.size()\n",
        "        inp = inp.permute(1, 0)           # seq_len x batch_size\n",
        "        target = target.permute(1, 0)     # seq_len x batch_size\n",
        "        h = self.init_hidden(batch_size)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            out, h = self.forward(inp[i], h)\n",
        "            loss += loss_fn(out, target[i])\n",
        "\n",
        "        return loss     # per batch\n",
        "\n",
        "    def batchPGLoss(self, inp, target, reward):\n",
        "        \"\"\"\n",
        "        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).\n",
        "        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/\n",
        "        Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size x seq_len\n",
        "            - reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding\n",
        "                      sentence)\n",
        "            inp should be target with <s> (start letter) prepended\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inp.size()\n",
        "        inp = inp.permute(1, 0)          # seq_len x batch_size\n",
        "        target = target.permute(1, 0)    # seq_len x batch_size\n",
        "        h = self.init_hidden(batch_size)\n",
        "\n",
        "        loss = 0\n",
        "        for i in range(seq_len):\n",
        "            out, h = self.forward(inp[i], h)\n",
        "            # TODO: should h be detached from graph (.detach())?\n",
        "            for j in range(batch_size):\n",
        "                loss += -out[j][target.data[i][j]]*reward[j]     # log(P(y_t|Y_1:Y_{t-1})) * Q\n",
        "\n",
        "        return loss/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3pIMHP1Rlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, gpu=False, dropout=0.2):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.gpu = gpu\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=dropout)\n",
        "        self.gru2hidden = nn.Linear(2*2*hidden_dim, hidden_dim)\n",
        "        self.dropout_linear = nn.Dropout(p=dropout)\n",
        "        self.hidden2out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = autograd.Variable(torch.zeros(2*2*1, batch_size, self.hidden_dim))\n",
        "\n",
        "        if self.gpu:\n",
        "            return h.cuda()\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input dim                                                # batch_size x seq_len\n",
        "        emb = self.embeddings(input)                               # batch_size x seq_len x embedding_dim\n",
        "        emb = emb.permute(1, 0, 2)                                 # seq_len x batch_size x embedding_dim\n",
        "        _, hidden = self.gru(emb, hidden)                          # 4 x batch_size x hidden_dim\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()              # batch_size x 4 x hidden_dim\n",
        "        out = self.gru2hidden(hidden.view(-1, 4*self.hidden_dim))  # batch_size x 4*hidden_dim\n",
        "        out = torch.tanh(out)\n",
        "        out = self.dropout_linear(out)\n",
        "        out = self.hidden2out(out)                                 # batch_size x 1\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "    def batchClassify(self, inp):\n",
        "        \"\"\"\n",
        "        Classifies a batch of sequences.\n",
        "        Inputs: inp\n",
        "            - inp: batch_size x seq_len\n",
        "        Returns: out\n",
        "            - out: batch_size ([0,1] score)\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.init_hidden(inp.size()[0])\n",
        "        out = self.forward(inp, h)\n",
        "        return out.view(-1)\n",
        "\n",
        "    def batchBCELoss(self, inp, target):\n",
        "        \"\"\"\n",
        "        Returns Binary Cross Entropy Loss for discriminator.\n",
        "         Inputs: inp, target\n",
        "            - inp: batch_size x seq_len\n",
        "            - target: batch_size (binary 1/0)\n",
        "        \"\"\"\n",
        "\n",
        "        loss_fn = nn.BCELoss()\n",
        "        h = self.init_hidden(inp.size()[0])\n",
        "        out = self.forward(inp, h)\n",
        "        return loss_fn(out, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI9bydIE1e0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_generator_batch(samples, start_letter=0, gpu=False):\n",
        "    \"\"\"\n",
        "    Takes samples (a batch) and returns\n",
        "    Inputs: samples, start_letter, cuda\n",
        "        - samples: batch_size x seq_len (Tensor with a sample in each row)\n",
        "    Returns: inp, target\n",
        "        - inp: batch_size x seq_len (same as target, but with start_letter prepended)\n",
        "        - target: batch_size x seq_len (Variable same as samples)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, seq_len = samples.size()\n",
        "\n",
        "    inp = torch.zeros(batch_size, seq_len)\n",
        "    target = samples\n",
        "    inp[:, 0] = start_letter\n",
        "    inp[:, 1:] = target[:, :seq_len-1]\n",
        "\n",
        "    inp = inp.type(torch.LongTensor)\n",
        "    target = target.type(torch.LongTensor)\n",
        "\n",
        "    if gpu:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "def prepare_discriminator_data(pos_samples, neg_samples, gpu=False):\n",
        "    \"\"\"\n",
        "    Takes positive (target) samples, negative (generator) samples and \n",
        "    prepares inp and target data for discriminator.\n",
        "    Inputs: pos_samples, neg_samples\n",
        "        - pos_samples: pos_size x seq_len\n",
        "        - neg_samples: neg_size x seq_len\n",
        "    Returns: inp, target\n",
        "        - inp: (pos_size + neg_size) x seq_len\n",
        "        - target: pos_size + neg_size (boolean 1/0)\n",
        "    \"\"\"\n",
        "\n",
        "    inp = torch.cat((pos_samples, neg_samples), 0).type(torch.LongTensor)\n",
        "    target = torch.ones(pos_samples.size()[0] + neg_samples.size()[0])\n",
        "    target[pos_samples.size()[0]:] = 0\n",
        "\n",
        "    # shuffle\n",
        "    perm = torch.randperm(target.size()[0])\n",
        "    target = target[perm]\n",
        "    inp = inp[perm]\n",
        "\n",
        "#    inp = Variable(inp)\n",
        "#    target = Variable(target)\n",
        "\n",
        "    if gpu:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "    return inp, target\n",
        "\n",
        "\n",
        "def batchwise_sample(gen, num_samples, batch_size):\n",
        "    \"\"\"\n",
        "    Sample num_samples samples batch_size samples at a time from gen.\n",
        "    Does not require gpu since gen.sample() takes care of that.\n",
        "    \"\"\"\n",
        "\n",
        "    samples = []\n",
        "    for i in range(int(ceil(num_samples/float(batch_size)))):\n",
        "        samples.append(gen.sample(batch_size))\n",
        "\n",
        "    return torch.cat(samples, 0)[:num_samples]\n",
        "\n",
        "def batchwise_oracle_nll(gen, oracle, num_samples, batch_size, max_seq_len, start_letter=0, gpu=False):\n",
        "    s = batchwise_sample(gen, num_samples, batch_size)\n",
        "    oracle_nll = 0\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        inp, target = prepare_generator_batch(s[i:i+batch_size], start_letter, gpu)\n",
        "        oracle_loss = oracle.batchNLLLoss(inp, target) / max_seq_len\n",
        "        oracle_nll += oracle_loss.data.item()\n",
        "\n",
        "    return oracle_nll/(num_samples/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJi2eS9t2BeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator_MLE(gen, gen_opt, oracle, real_data_samples, epochs):\n",
        "    \"\"\"\n",
        "    Max Likelihood Pretraining for the generator\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch %d : ' % (epoch + 1), end='')\n",
        "        sys.stdout.flush()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
        "            inp, target = prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
        "                                                          gpu=CUDA)\n",
        "            gen_opt.zero_grad()\n",
        "            loss = gen.batchNLLLoss(inp, target)\n",
        "            loss.backward()\n",
        "            gen_opt.step()\n",
        "\n",
        "            total_loss += loss.data.item()\n",
        "\n",
        "            if (i / BATCH_SIZE) % ceil(\n",
        "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
        "                print('.', end='')\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        # each loss in a batch is loss per sample\n",
        "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
        "\n",
        "        # sample from generator and compute oracle NLL\n",
        "        oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
        "        \n",
        "        loss_g.append(oracle_loss)\n",
        "\n",
        "        print(' average_train_NLL = %.4f, oracle_sample_NLL = %.4f' % (total_loss, oracle_loss))\n",
        "\n",
        "\n",
        "def train_generator_PG(gen, gen_opt, oracle, dis, num_batches):\n",
        "    \"\"\"\n",
        "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
        "    Training is done for num_batches batches.\n",
        "    \"\"\"\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
        "        inp, target = prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
        "        rewards = dis.batchClassify(target)\n",
        "\n",
        "        gen_opt.zero_grad()\n",
        "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
        "        pg_loss.backward()\n",
        "        gen_opt.step()\n",
        "\n",
        "    # sample from generator and compute oracle NLL\n",
        "    oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                                   start_letter=START_LETTER, gpu=CUDA)\n",
        "\n",
        "    loss_g.append(oracle_loss)\n",
        "    \n",
        "    print(' oracle_sample_NLL = %.4f' % oracle_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nFTuxWK2fn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, oracle, d_steps, epochs):\n",
        "    \"\"\"\n",
        "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
        "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    # generating a small validation set before training (using oracle and generator)\n",
        "    pos_val = oracle.sample(100)\n",
        "    neg_val = generator.sample(100)\n",
        "    val_inp, val_target = prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
        "\n",
        "    for d_step in range(d_steps):\n",
        "        s = batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
        "#        print(s.shape, real_data_samples.shape)\n",
        "        dis_inp, dis_target = prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
        "        for epoch in range(epochs):\n",
        "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
        "            sys.stdout.flush()\n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "\n",
        "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE): #2 * POS_NEG_SAMPLES because both pos \n",
        "                                                                #and neg samples included in dis_inp\n",
        "                inp, target = dis_inp[i:i + BATCH_SIZE], dis_target[i:i + BATCH_SIZE]\n",
        "                dis_opt.zero_grad()\n",
        "                out = discriminator.batchClassify(inp)\n",
        "                loss_fn = nn.BCELoss()\n",
        "                loss = loss_fn(out, target)\n",
        "                loss.backward()\n",
        "                dis_opt.step()\n",
        "\n",
        "                total_loss += loss.data.item()\n",
        "                total_acc += torch.sum((out>0.5)==(target>0.5)).data.item()\n",
        "\n",
        "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
        "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
        "                    print('.', end='')\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
        "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
        "\n",
        "            val_pred = discriminator.batchClassify(val_inp)\n",
        "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
        "                total_loss, total_acc, torch.sum((val_pred>0.5)==(val_target>0.5)).data.item()/200.))\n",
        "            \n",
        "            loss_d.append(total_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6kKYPcBhm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUDA = True\n",
        "\n",
        "#Fixed Params\n",
        "VOCAB_SIZE = 22 #Starting Letter + 20 AA + Padding\n",
        "MAX_SEQ_LEN = 18 #2000 kDa / 110 kDa = 18\n",
        "START_LETTER = 0 \n",
        "POS_NEG_SAMPLES = len(all_data) #Size of AVPDb dataset\n",
        "torch.manual_seed(11)\n",
        "\n",
        "#Variables\n",
        "BATCH_SIZE = 512 \n",
        "ADV_TRAIN_EPOCHS = 100 \n",
        "\n",
        "#Generator Parameters\n",
        "MLE_TRAIN_EPOCHS = 100\n",
        "GEN_EMBEDDING_DIM = 3\n",
        "GEN_HIDDEN_DIM = 32\n",
        "NUM_PG_BATCHES = 1\n",
        "GEN_lr = 1e-1\n",
        "\n",
        "#Discriminator Parameters\n",
        "DIS_EMBEDDING_DIM = 5            \n",
        "DIS_HIDDEN_DIM = 32\n",
        "D_STEPS = 50\n",
        "D_EPOCHS = 2\n",
        "ADV_D_EPOCHS = 3\n",
        "ADV_D_STEPS = 5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y8IoAPo2ktX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f50daa3-10be-433e-f23e-fff44ac58834"
      },
      "source": [
        "#Used to store output of cell\n",
        "\n",
        "# MAIN\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # a new oracle can be generated by passing oracle_init=True in the generator constructor\n",
        "    oracle = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA, oracle_init=True)\n",
        "\n",
        "    #Initialize Generator and Discriminator\n",
        "    gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
        "    dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
        "\n",
        "    loss_g = []\n",
        "    loss_d = []\n",
        "\n",
        "    if CUDA:\n",
        "      oracle = oracle.cuda()\n",
        "      gen = gen.cuda()\n",
        "      dis = dis.cuda()\n",
        "#      oracle_samples = oracle_samples.cuda()\n",
        "\n",
        "    #Converts Vectorized Inputs to Tensor\n",
        "    oracle_samples = torch.Tensor(all_data).cuda()\n",
        "\n",
        "    # GENERATOR MLE TRAINING\n",
        "    print('Starting Generator MLE Training...')\n",
        "    gen_optimizer = optim.Adam(gen.parameters(), lr = GEN_lr)\n",
        "    train_generator_MLE(gen, gen_optimizer, oracle, torch.Tensor(all_data), MLE_TRAIN_EPOCHS)\n",
        "\n",
        "    # torch.save(gen.state_dict(), pretrained_gen_path)\n",
        "    # gen.load_state_dict(torch.load(pretrained_gen_path))\n",
        "\n",
        "    # PRETRAIN DISCRIMINATOR\n",
        "    print('\\nStarting Discriminator Training...')\n",
        "    dis_optimizer = optim.Adagrad(dis.parameters())\n",
        "    train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, D_STEPS, D_EPOCHS)\n",
        "\n",
        "    # ADVERSARIAL TRAINING\n",
        "    print('\\nStarting Adversarial Training...')\n",
        "    oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                               start_letter=START_LETTER, gpu=CUDA)\n",
        "    print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
        "\n",
        "    for epoch in range(ADV_TRAIN_EPOCHS):\n",
        "        print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
        "        # TRAIN GENERATOR\n",
        "        print('\\nAdversarial Training Generator : ', end='')\n",
        "        sys.stdout.flush()\n",
        "        train_generator_PG(gen, gen_optimizer, oracle, dis, NUM_PG_BATCHES)\n",
        "\n",
        "        # TRAIN DISCRIMINATOR\n",
        "        print('\\nAdversarial Training Discriminator : ')\n",
        "        train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, ADV_D_STEPS, ADV_D_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting Generator MLE Training...\n",
            "epoch 1 : ..... average_train_NLL = 3.3153, oracle_sample_NLL = 10.0874\n",
            "epoch 2 : ..... average_train_NLL = 2.5697, oracle_sample_NLL = 10.8635\n",
            "epoch 3 : ..... average_train_NLL = 2.3919, oracle_sample_NLL = 10.4072\n",
            "epoch 4 : ..... average_train_NLL = 2.2985, oracle_sample_NLL = 10.2430\n",
            "epoch 5 : ..... average_train_NLL = 2.2501, oracle_sample_NLL = 10.5561\n",
            "epoch 6 : ..... average_train_NLL = 2.2094, oracle_sample_NLL = 10.8182\n",
            "epoch 7 : ..... average_train_NLL = 2.1662, oracle_sample_NLL = 10.1169\n",
            "epoch 8 : ..... average_train_NLL = 2.1466, oracle_sample_NLL = 10.5022\n",
            "epoch 9 : ..... average_train_NLL = 2.1366, oracle_sample_NLL = 10.8195\n",
            "epoch 10 : ..... average_train_NLL = 2.1125, oracle_sample_NLL = 10.2257\n",
            "epoch 11 : ..... average_train_NLL = 2.0597, oracle_sample_NLL = 10.8130\n",
            "epoch 12 : ..... average_train_NLL = 2.0222, oracle_sample_NLL = 10.4392\n",
            "epoch 13 : ..... average_train_NLL = 2.0135, oracle_sample_NLL = 10.6219\n",
            "epoch 14 : ..... average_train_NLL = 2.0020, oracle_sample_NLL = 10.4416\n",
            "epoch 15 : ..... average_train_NLL = 1.9935, oracle_sample_NLL = 10.6767\n",
            "epoch 16 : ..... average_train_NLL = 1.9832, oracle_sample_NLL = 10.5557\n",
            "epoch 17 : ..... average_train_NLL = 2.0049, oracle_sample_NLL = 10.4231\n",
            "epoch 18 : ..... average_train_NLL = 2.1017, oracle_sample_NLL = 10.5397\n",
            "epoch 19 : ..... average_train_NLL = 2.1327, oracle_sample_NLL = 10.2180\n",
            "epoch 20 : ..... average_train_NLL = 2.1169, oracle_sample_NLL = 10.8645\n",
            "epoch 21 : ..... average_train_NLL = 2.0732, oracle_sample_NLL = 10.4914\n",
            "epoch 22 : ..... average_train_NLL = 2.0678, oracle_sample_NLL = 10.6870\n",
            "epoch 23 : ..... average_train_NLL = 2.0358, oracle_sample_NLL = 10.3432\n",
            "epoch 24 : ..... average_train_NLL = 2.0255, oracle_sample_NLL = 10.6174\n",
            "epoch 25 : ..... average_train_NLL = 1.9996, oracle_sample_NLL = 10.5563\n",
            "epoch 26 : ..... average_train_NLL = 1.9905, oracle_sample_NLL = 10.4120\n",
            "epoch 27 : ..... average_train_NLL = 1.9675, oracle_sample_NLL = 10.3801\n",
            "epoch 28 : ..... average_train_NLL = 1.9683, oracle_sample_NLL = 10.6217\n",
            "epoch 29 : ..... average_train_NLL = 1.9370, oracle_sample_NLL = 10.4373\n",
            "epoch 30 : ..... average_train_NLL = 1.9257, oracle_sample_NLL = 10.4839\n",
            "epoch 31 : ..... average_train_NLL = 1.9247, oracle_sample_NLL = 10.3686\n",
            "epoch 32 : ..... average_train_NLL = 1.9206, oracle_sample_NLL = 10.4762\n",
            "epoch 33 : ..... average_train_NLL = 1.8990, oracle_sample_NLL = 10.3052\n",
            "epoch 34 : ..... average_train_NLL = 1.8807, oracle_sample_NLL = 10.4573\n",
            "epoch 35 : ..... average_train_NLL = 1.8584, oracle_sample_NLL = 10.5554\n",
            "epoch 36 : ..... average_train_NLL = 1.8500, oracle_sample_NLL = 10.6488\n",
            "epoch 37 : ..... average_train_NLL = 1.8386, oracle_sample_NLL = 10.4034\n",
            "epoch 38 : ..... average_train_NLL = 1.8308, oracle_sample_NLL = 10.6405\n",
            "epoch 39 : ..... average_train_NLL = 1.8148, oracle_sample_NLL = 10.3978\n",
            "epoch 40 : ..... average_train_NLL = 1.8062, oracle_sample_NLL = 10.5029\n",
            "epoch 41 : ..... average_train_NLL = 1.8185, oracle_sample_NLL = 10.0874\n",
            "epoch 42 : ..... average_train_NLL = 1.8321, oracle_sample_NLL = 10.3639\n",
            "epoch 43 : ..... average_train_NLL = 1.8054, oracle_sample_NLL = 10.6428\n",
            "epoch 44 : ..... average_train_NLL = 1.7970, oracle_sample_NLL = 10.1939\n",
            "epoch 45 : ..... average_train_NLL = 1.7870, oracle_sample_NLL = 10.4634\n",
            "epoch 46 : ..... average_train_NLL = 1.7809, oracle_sample_NLL = 10.4386\n",
            "epoch 47 : ..... average_train_NLL = 1.7731, oracle_sample_NLL = 10.4909\n",
            "epoch 48 : ..... average_train_NLL = 1.7732, oracle_sample_NLL = 10.1367\n",
            "epoch 49 : ..... average_train_NLL = 1.7718, oracle_sample_NLL = 10.5534\n",
            "epoch 50 : ..... average_train_NLL = 1.7785, oracle_sample_NLL = 10.3669\n",
            "epoch 51 : ..... average_train_NLL = 1.7989, oracle_sample_NLL = 10.2742\n",
            "epoch 52 : ..... average_train_NLL = 1.7785, oracle_sample_NLL = 10.4858\n",
            "epoch 53 : ..... average_train_NLL = 1.7744, oracle_sample_NLL = 10.3676\n",
            "epoch 54 : ..... average_train_NLL = 1.7705, oracle_sample_NLL = 10.7053\n",
            "epoch 55 : ..... average_train_NLL = 1.7600, oracle_sample_NLL = 10.3840\n",
            "epoch 56 : ..... average_train_NLL = 1.7535, oracle_sample_NLL = 10.5679\n",
            "epoch 57 : ..... average_train_NLL = 1.7521, oracle_sample_NLL = 10.2503\n",
            "epoch 58 : ..... average_train_NLL = 1.7471, oracle_sample_NLL = 10.2606\n",
            "epoch 59 : ..... average_train_NLL = 1.7345, oracle_sample_NLL = 10.4276\n",
            "epoch 60 : ..... average_train_NLL = 1.7316, oracle_sample_NLL = 10.5696\n",
            "epoch 61 : ..... average_train_NLL = 1.7334, oracle_sample_NLL = 10.3193\n",
            "epoch 62 : ..... average_train_NLL = 1.7238, oracle_sample_NLL = 10.3381\n",
            "epoch 63 : ..... average_train_NLL = 1.7288, oracle_sample_NLL = 10.4666\n",
            "epoch 64 : ..... average_train_NLL = 1.7283, oracle_sample_NLL = 10.2297\n",
            "epoch 65 : ..... average_train_NLL = 1.7153, oracle_sample_NLL = 10.5557\n",
            "epoch 66 : ..... average_train_NLL = 1.7228, oracle_sample_NLL = 10.2226\n",
            "epoch 67 : ..... average_train_NLL = 1.7202, oracle_sample_NLL = 10.4145\n",
            "epoch 68 : ..... average_train_NLL = 1.7197, oracle_sample_NLL = 10.3208\n",
            "epoch 69 : ..... average_train_NLL = 1.7170, oracle_sample_NLL = 10.3794\n",
            "epoch 70 : ..... average_train_NLL = 1.7398, oracle_sample_NLL = 10.3366\n",
            "epoch 71 : ..... average_train_NLL = 1.7413, oracle_sample_NLL = 10.3141\n",
            "epoch 72 : ..... average_train_NLL = 1.7355, oracle_sample_NLL = 10.7130\n",
            "epoch 73 : ..... average_train_NLL = 1.8155, oracle_sample_NLL = 10.5530\n",
            "epoch 74 : ..... average_train_NLL = 1.9593, oracle_sample_NLL = 10.4688\n",
            "epoch 75 : ..... average_train_NLL = 2.6856, oracle_sample_NLL = 10.2094\n",
            "epoch 76 : ..... average_train_NLL = 3.0140, oracle_sample_NLL = 10.4995\n",
            "epoch 77 : ..... average_train_NLL = 3.0790, oracle_sample_NLL = 10.6508\n",
            "epoch 78 : ..... average_train_NLL = 2.8848, oracle_sample_NLL = 10.1693\n",
            "epoch 79 : ..... average_train_NLL = 2.6677, oracle_sample_NLL = 10.8596\n",
            "epoch 80 : ..... average_train_NLL = 2.6076, oracle_sample_NLL = 11.5146\n",
            "epoch 81 : ..... average_train_NLL = 2.4892, oracle_sample_NLL = 10.6499\n",
            "epoch 82 : ..... average_train_NLL = 2.3890, oracle_sample_NLL = 10.6211\n",
            "epoch 83 : ..... average_train_NLL = 2.3867, oracle_sample_NLL = 10.4048\n",
            "epoch 84 : ..... average_train_NLL = 2.3107, oracle_sample_NLL = 11.1608\n",
            "epoch 85 : ..... average_train_NLL = 2.2894, oracle_sample_NLL = 10.6991\n",
            "epoch 86 : ..... average_train_NLL = 2.2814, oracle_sample_NLL = 10.1924\n",
            "epoch 87 : ..... average_train_NLL = 2.2454, oracle_sample_NLL = 10.7744\n",
            "epoch 88 : ..... average_train_NLL = 2.2336, oracle_sample_NLL = 10.8530\n",
            "epoch 89 : ..... average_train_NLL = 2.2258, oracle_sample_NLL = 10.5815\n",
            "epoch 90 : ..... average_train_NLL = 2.2107, oracle_sample_NLL = 10.5101\n",
            "epoch 91 : ..... average_train_NLL = 2.2073, oracle_sample_NLL = 10.6760\n",
            "epoch 92 : ..... average_train_NLL = 2.1925, oracle_sample_NLL = 10.8041\n",
            "epoch 93 : ..... average_train_NLL = 2.1886, oracle_sample_NLL = 10.5680\n",
            "epoch 94 : ..... average_train_NLL = 2.1808, oracle_sample_NLL = 10.5421\n",
            "epoch 95 : ..... average_train_NLL = 2.1727, oracle_sample_NLL = 10.8808\n",
            "epoch 96 : ..... average_train_NLL = 2.1696, oracle_sample_NLL = 10.6510\n",
            "epoch 97 : ..... average_train_NLL = 2.1649, oracle_sample_NLL = 10.6151\n",
            "epoch 98 : ..... average_train_NLL = 2.1581, oracle_sample_NLL = 10.4828\n",
            "epoch 99 : ..... average_train_NLL = 2.1572, oracle_sample_NLL = 10.8470\n",
            "epoch 100 : ..... average_train_NLL = 2.1535, oracle_sample_NLL = 10.4811\n",
            "\n",
            "Starting Discriminator Training...\n",
            "d-step 1 epoch 1 : ......... average_loss = 0.6882, train_acc = 0.5474, val_acc = 0.4450\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.6633, train_acc = 0.5772, val_acc = 0.4200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.6507, train_acc = 0.5908, val_acc = 0.4000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.6465, train_acc = 0.5988, val_acc = 0.3600\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.6397, train_acc = 0.6102, val_acc = 0.4150\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.6316, train_acc = 0.6112, val_acc = 0.4200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.6364, train_acc = 0.6081, val_acc = 0.3650\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.6222, train_acc = 0.6357, val_acc = 0.3850\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.6258, train_acc = 0.6333, val_acc = 0.4350\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.6171, train_acc = 0.6445, val_acc = 0.4450\n",
            "d-step 6 epoch 1 : ......... average_loss = 0.6100, train_acc = 0.6467, val_acc = 0.4150\n",
            "d-step 6 epoch 2 : ......... average_loss = 0.6091, train_acc = 0.6513, val_acc = 0.4200\n",
            "d-step 7 epoch 1 : ......... average_loss = 0.6267, train_acc = 0.6438, val_acc = 0.3650\n",
            "d-step 7 epoch 2 : ......... average_loss = 0.6236, train_acc = 0.6459, val_acc = 0.3950\n",
            "d-step 8 epoch 1 : ......... average_loss = 0.5953, train_acc = 0.6673, val_acc = 0.4400\n",
            "d-step 8 epoch 2 : ......... average_loss = 0.5849, train_acc = 0.6680, val_acc = 0.4300\n",
            "d-step 9 epoch 1 : ......... average_loss = 0.6039, train_acc = 0.6629, val_acc = 0.4600\n",
            "d-step 9 epoch 2 : ......... average_loss = 0.5912, train_acc = 0.6668, val_acc = 0.4400\n",
            "d-step 10 epoch 1 : ......... average_loss = 0.5735, train_acc = 0.6722, val_acc = 0.4250\n",
            "d-step 10 epoch 2 : ......... average_loss = 0.5614, train_acc = 0.6867, val_acc = 0.4400\n",
            "d-step 11 epoch 1 : ......... average_loss = 0.5915, train_acc = 0.6768, val_acc = 0.4450\n",
            "d-step 11 epoch 2 : ......... average_loss = 0.5804, train_acc = 0.6770, val_acc = 0.4750\n",
            "d-step 12 epoch 1 : ......... average_loss = 0.5863, train_acc = 0.6889, val_acc = 0.4500\n",
            "d-step 12 epoch 2 : ......... average_loss = 0.5797, train_acc = 0.6870, val_acc = 0.4800\n",
            "d-step 13 epoch 1 : ......... average_loss = 0.5853, train_acc = 0.6935, val_acc = 0.5200\n",
            "d-step 13 epoch 2 : ......... average_loss = 0.5676, train_acc = 0.7018, val_acc = 0.4950\n",
            "d-step 14 epoch 1 : ......... average_loss = 0.5619, train_acc = 0.7059, val_acc = 0.4700\n",
            "d-step 14 epoch 2 : ......... average_loss = 0.5522, train_acc = 0.7137, val_acc = 0.4500\n",
            "d-step 15 epoch 1 : ......... average_loss = 0.5726, train_acc = 0.7105, val_acc = 0.3650\n",
            "d-step 15 epoch 2 : ......... average_loss = 0.5683, train_acc = 0.7081, val_acc = 0.3700\n",
            "d-step 16 epoch 1 : ......... average_loss = 0.5509, train_acc = 0.7220, val_acc = 0.4500\n",
            "d-step 16 epoch 2 : ......... average_loss = 0.5424, train_acc = 0.7263, val_acc = 0.4400\n",
            "d-step 17 epoch 1 : ......... average_loss = 0.5340, train_acc = 0.7096, val_acc = 0.4700\n",
            "d-step 17 epoch 2 : ......... average_loss = 0.5227, train_acc = 0.7203, val_acc = 0.4850\n",
            "d-step 18 epoch 1 : ......... average_loss = 0.5484, train_acc = 0.7212, val_acc = 0.4000\n",
            "d-step 18 epoch 2 : ......... average_loss = 0.5443, train_acc = 0.7173, val_acc = 0.4300\n",
            "d-step 19 epoch 1 : ......... average_loss = 0.5440, train_acc = 0.7227, val_acc = 0.4600\n",
            "d-step 19 epoch 2 : ......... average_loss = 0.5389, train_acc = 0.7237, val_acc = 0.4650\n",
            "d-step 20 epoch 1 : ......... average_loss = 0.5532, train_acc = 0.7183, val_acc = 0.4700\n",
            "d-step 20 epoch 2 : ......... average_loss = 0.5451, train_acc = 0.7241, val_acc = 0.4750\n",
            "d-step 21 epoch 1 : ......... average_loss = 0.5353, train_acc = 0.7256, val_acc = 0.4500\n",
            "d-step 21 epoch 2 : ......... average_loss = 0.5343, train_acc = 0.7108, val_acc = 0.4350\n",
            "d-step 22 epoch 1 : ......... average_loss = 0.5334, train_acc = 0.7244, val_acc = 0.5250\n",
            "d-step 22 epoch 2 : ......... average_loss = 0.5217, train_acc = 0.7331, val_acc = 0.5150\n",
            "d-step 23 epoch 1 : ......... average_loss = 0.5329, train_acc = 0.7275, val_acc = 0.4850\n",
            "d-step 23 epoch 2 : ......... average_loss = 0.5248, train_acc = 0.7329, val_acc = 0.4700\n",
            "d-step 24 epoch 1 : ......... average_loss = 0.5401, train_acc = 0.7292, val_acc = 0.5450\n",
            "d-step 24 epoch 2 : ......... average_loss = 0.5326, train_acc = 0.7229, val_acc = 0.5800\n",
            "d-step 25 epoch 1 : ......... average_loss = 0.5054, train_acc = 0.7373, val_acc = 0.4600\n",
            "d-step 25 epoch 2 : ......... average_loss = 0.4971, train_acc = 0.7409, val_acc = 0.4550\n",
            "d-step 26 epoch 1 : ......... average_loss = 0.5191, train_acc = 0.7373, val_acc = 0.5000\n",
            "d-step 26 epoch 2 : ......... average_loss = 0.5100, train_acc = 0.7392, val_acc = 0.4850\n",
            "d-step 27 epoch 1 : ......... average_loss = 0.5434, train_acc = 0.7324, val_acc = 0.5450\n",
            "d-step 27 epoch 2 : ......... average_loss = 0.5294, train_acc = 0.7353, val_acc = 0.5250\n",
            "d-step 28 epoch 1 : ......... average_loss = 0.4914, train_acc = 0.7552, val_acc = 0.5100\n",
            "d-step 28 epoch 2 : ......... average_loss = 0.4837, train_acc = 0.7586, val_acc = 0.5250\n",
            "d-step 29 epoch 1 : ......... average_loss = 0.5023, train_acc = 0.7530, val_acc = 0.5350\n",
            "d-step 29 epoch 2 : ......... average_loss = 0.4922, train_acc = 0.7545, val_acc = 0.5300\n",
            "d-step 30 epoch 1 : ......... average_loss = 0.4953, train_acc = 0.7380, val_acc = 0.5250\n",
            "d-step 30 epoch 2 : ......... average_loss = 0.4872, train_acc = 0.7513, val_acc = 0.5400\n",
            "d-step 31 epoch 1 : ......... average_loss = 0.4981, train_acc = 0.7530, val_acc = 0.5300\n",
            "d-step 31 epoch 2 : ......... average_loss = 0.4923, train_acc = 0.7581, val_acc = 0.5100\n",
            "d-step 32 epoch 1 : ......... average_loss = 0.4863, train_acc = 0.7661, val_acc = 0.5150\n",
            "d-step 32 epoch 2 : ......... average_loss = 0.4846, train_acc = 0.7623, val_acc = 0.5050\n",
            "d-step 33 epoch 1 : ......... average_loss = 0.5037, train_acc = 0.7535, val_acc = 0.4500\n",
            "d-step 33 epoch 2 : ......... average_loss = 0.4883, train_acc = 0.7518, val_acc = 0.4350\n",
            "d-step 34 epoch 1 : ......... average_loss = 0.4966, train_acc = 0.7564, val_acc = 0.5550\n",
            "d-step 34 epoch 2 : ......... average_loss = 0.4890, train_acc = 0.7596, val_acc = 0.5400\n",
            "d-step 35 epoch 1 : ......... average_loss = 0.5235, train_acc = 0.7562, val_acc = 0.5550\n",
            "d-step 35 epoch 2 : ......... average_loss = 0.5129, train_acc = 0.7567, val_acc = 0.5200\n",
            "d-step 36 epoch 1 : ......... average_loss = 0.4868, train_acc = 0.7540, val_acc = 0.5000\n",
            "d-step 36 epoch 2 : ......... average_loss = 0.4769, train_acc = 0.7601, val_acc = 0.5350\n",
            "d-step 37 epoch 1 : ......... average_loss = 0.4710, train_acc = 0.7647, val_acc = 0.5200\n",
            "d-step 37 epoch 2 : ......... average_loss = 0.4655, train_acc = 0.7649, val_acc = 0.5200\n",
            "d-step 38 epoch 1 : ......... average_loss = 0.4770, train_acc = 0.7659, val_acc = 0.5050\n",
            "d-step 38 epoch 2 : ......... average_loss = 0.4846, train_acc = 0.7591, val_acc = 0.5250\n",
            "d-step 39 epoch 1 : ......... average_loss = 0.4782, train_acc = 0.7610, val_acc = 0.5200\n",
            "d-step 39 epoch 2 : ......... average_loss = 0.4661, train_acc = 0.7698, val_acc = 0.4900\n",
            "d-step 40 epoch 1 : ......... average_loss = 0.4818, train_acc = 0.7732, val_acc = 0.5300\n",
            "d-step 40 epoch 2 : ......... average_loss = 0.4707, train_acc = 0.7790, val_acc = 0.5250\n",
            "d-step 41 epoch 1 : ......... average_loss = 0.4608, train_acc = 0.7812, val_acc = 0.4700\n",
            "d-step 41 epoch 2 : ......... average_loss = 0.4602, train_acc = 0.7839, val_acc = 0.4750\n",
            "d-step 42 epoch 1 : ......... average_loss = 0.4733, train_acc = 0.7761, val_acc = 0.5350\n",
            "d-step 42 epoch 2 : ......... average_loss = 0.4663, train_acc = 0.7795, val_acc = 0.5600\n",
            "d-step 43 epoch 1 : ......... average_loss = 0.4539, train_acc = 0.7715, val_acc = 0.5250\n",
            "d-step 43 epoch 2 : ......... average_loss = 0.4499, train_acc = 0.7771, val_acc = 0.5000\n",
            "d-step 44 epoch 1 : ......... average_loss = 0.4976, train_acc = 0.7754, val_acc = 0.5050\n",
            "d-step 44 epoch 2 : ......... average_loss = 0.5015, train_acc = 0.7674, val_acc = 0.4900\n",
            "d-step 45 epoch 1 : ......... average_loss = 0.4866, train_acc = 0.7652, val_acc = 0.5050\n",
            "d-step 45 epoch 2 : ......... average_loss = 0.4706, train_acc = 0.7780, val_acc = 0.5300\n",
            "d-step 46 epoch 1 : ......... average_loss = 0.4518, train_acc = 0.7805, val_acc = 0.5600\n",
            "d-step 46 epoch 2 : ......... average_loss = 0.4484, train_acc = 0.7858, val_acc = 0.5650\n",
            "d-step 47 epoch 1 : ......... average_loss = 0.4415, train_acc = 0.7963, val_acc = 0.5200\n",
            "d-step 47 epoch 2 : ......... average_loss = 0.4352, train_acc = 0.7914, val_acc = 0.5100\n",
            "d-step 48 epoch 1 : ......... average_loss = 0.4802, train_acc = 0.7761, val_acc = 0.5800\n",
            "d-step 48 epoch 2 : ......... average_loss = 0.4542, train_acc = 0.7856, val_acc = 0.5700\n",
            "d-step 49 epoch 1 : ......... average_loss = 0.4500, train_acc = 0.7844, val_acc = 0.5200\n",
            "d-step 49 epoch 2 : ......... average_loss = 0.4396, train_acc = 0.7817, val_acc = 0.5100\n",
            "d-step 50 epoch 1 : ......... average_loss = 0.4541, train_acc = 0.7943, val_acc = 0.5250\n",
            "d-step 50 epoch 2 : ......... average_loss = 0.4465, train_acc = 0.7950, val_acc = 0.5400\n",
            "\n",
            "Starting Adversarial Training...\n",
            "\n",
            "Initial Oracle Sample Loss : 10.5305\n",
            "\n",
            "--------\n",
            "EPOCH 1\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8145\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.4002, train_acc = 0.8130, val_acc = 0.5050\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3848, train_acc = 0.8159, val_acc = 0.5400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3826, train_acc = 0.8196, val_acc = 0.5400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.4069, train_acc = 0.8184, val_acc = 0.5100\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.4040, train_acc = 0.8111, val_acc = 0.5100\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3936, train_acc = 0.8101, val_acc = 0.5150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3992, train_acc = 0.8222, val_acc = 0.5350\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3926, train_acc = 0.8227, val_acc = 0.5150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3872, train_acc = 0.8252, val_acc = 0.5100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.4198, train_acc = 0.8203, val_acc = 0.5200\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.4046, train_acc = 0.8288, val_acc = 0.5000\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3999, train_acc = 0.8320, val_acc = 0.5100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.4123, train_acc = 0.8174, val_acc = 0.5200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3916, train_acc = 0.8201, val_acc = 0.5200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3975, train_acc = 0.8186, val_acc = 0.5450\n",
            "\n",
            "--------\n",
            "EPOCH 2\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5988\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3888, train_acc = 0.8334, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3838, train_acc = 0.8341, val_acc = 0.5950\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3780, train_acc = 0.8371, val_acc = 0.5950\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3575, train_acc = 0.8358, val_acc = 0.5450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3463, train_acc = 0.8434, val_acc = 0.5500\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3455, train_acc = 0.8424, val_acc = 0.5450\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3692, train_acc = 0.8407, val_acc = 0.5800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3739, train_acc = 0.8375, val_acc = 0.5500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3701, train_acc = 0.8368, val_acc = 0.5750\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3751, train_acc = 0.8463, val_acc = 0.5900\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3581, train_acc = 0.8536, val_acc = 0.5850\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3487, train_acc = 0.8533, val_acc = 0.5650\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3498, train_acc = 0.8456, val_acc = 0.6250\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3401, train_acc = 0.8482, val_acc = 0.6100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3396, train_acc = 0.8507, val_acc = 0.6200\n",
            "\n",
            "--------\n",
            "EPOCH 3\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.4411\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3467, train_acc = 0.8601, val_acc = 0.5150\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3442, train_acc = 0.8652, val_acc = 0.5150\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3259, train_acc = 0.8681, val_acc = 0.5150\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3103, train_acc = 0.8630, val_acc = 0.4950\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2959, train_acc = 0.8674, val_acc = 0.4950\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2945, train_acc = 0.8723, val_acc = 0.5200\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3187, train_acc = 0.8613, val_acc = 0.5100\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3196, train_acc = 0.8611, val_acc = 0.4950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3100, train_acc = 0.8606, val_acc = 0.5000\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3227, train_acc = 0.8650, val_acc = 0.4750\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3185, train_acc = 0.8713, val_acc = 0.4750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3178, train_acc = 0.8694, val_acc = 0.4700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3408, train_acc = 0.8604, val_acc = 0.5150\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3278, train_acc = 0.8643, val_acc = 0.5300\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3285, train_acc = 0.8650, val_acc = 0.5250\n",
            "\n",
            "--------\n",
            "EPOCH 4\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5294\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2677, train_acc = 0.8864, val_acc = 0.5450\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2635, train_acc = 0.8873, val_acc = 0.5300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2619, train_acc = 0.8839, val_acc = 0.5300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2772, train_acc = 0.8861, val_acc = 0.6150\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2659, train_acc = 0.8878, val_acc = 0.5900\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2592, train_acc = 0.8883, val_acc = 0.5950\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2694, train_acc = 0.8878, val_acc = 0.5500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2654, train_acc = 0.8851, val_acc = 0.5600\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2555, train_acc = 0.8900, val_acc = 0.5650\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2813, train_acc = 0.8851, val_acc = 0.5550\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2793, train_acc = 0.8832, val_acc = 0.5500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2777, train_acc = 0.8793, val_acc = 0.5250\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2877, train_acc = 0.8822, val_acc = 0.5650\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2832, train_acc = 0.8803, val_acc = 0.5700\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2675, train_acc = 0.8849, val_acc = 0.5700\n",
            "\n",
            "--------\n",
            "EPOCH 5\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6847\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2848, train_acc = 0.8885, val_acc = 0.5400\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2679, train_acc = 0.8888, val_acc = 0.5400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2654, train_acc = 0.8902, val_acc = 0.5300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2634, train_acc = 0.8885, val_acc = 0.5400\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2677, train_acc = 0.8876, val_acc = 0.5400\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2637, train_acc = 0.8868, val_acc = 0.5350\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2507, train_acc = 0.8895, val_acc = 0.5450\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2544, train_acc = 0.8922, val_acc = 0.5350\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2420, train_acc = 0.8961, val_acc = 0.5400\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2717, train_acc = 0.8946, val_acc = 0.5600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2690, train_acc = 0.8968, val_acc = 0.5500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2652, train_acc = 0.8893, val_acc = 0.5650\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2721, train_acc = 0.8941, val_acc = 0.5550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2745, train_acc = 0.8936, val_acc = 0.5650\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2704, train_acc = 0.8917, val_acc = 0.5550\n",
            "\n",
            "--------\n",
            "EPOCH 6\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5394\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2675, train_acc = 0.9034, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2689, train_acc = 0.8915, val_acc = 0.5950\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2591, train_acc = 0.8980, val_acc = 0.6350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2625, train_acc = 0.9000, val_acc = 0.6000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2566, train_acc = 0.8968, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2535, train_acc = 0.8987, val_acc = 0.6100\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2413, train_acc = 0.8958, val_acc = 0.5950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2388, train_acc = 0.8997, val_acc = 0.5950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2352, train_acc = 0.8995, val_acc = 0.5700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2430, train_acc = 0.8973, val_acc = 0.6050\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2400, train_acc = 0.9019, val_acc = 0.6000\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2373, train_acc = 0.9007, val_acc = 0.5950\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2612, train_acc = 0.9012, val_acc = 0.6200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2491, train_acc = 0.9043, val_acc = 0.6200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2436, train_acc = 0.9019, val_acc = 0.6200\n",
            "\n",
            "--------\n",
            "EPOCH 7\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8218\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2647, train_acc = 0.8958, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2623, train_acc = 0.8934, val_acc = 0.6250\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2562, train_acc = 0.8944, val_acc = 0.6050\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2561, train_acc = 0.8927, val_acc = 0.5950\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2613, train_acc = 0.8934, val_acc = 0.5950\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2501, train_acc = 0.8917, val_acc = 0.5950\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2700, train_acc = 0.8951, val_acc = 0.5600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2650, train_acc = 0.8963, val_acc = 0.5800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2557, train_acc = 0.8927, val_acc = 0.5850\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2634, train_acc = 0.8888, val_acc = 0.6350\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2472, train_acc = 0.8949, val_acc = 0.6200\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2512, train_acc = 0.8944, val_acc = 0.6200\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2354, train_acc = 0.9051, val_acc = 0.6250\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2387, train_acc = 0.9017, val_acc = 0.6200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2303, train_acc = 0.9029, val_acc = 0.6300\n",
            "\n",
            "--------\n",
            "EPOCH 8\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5432\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2580, train_acc = 0.9014, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2507, train_acc = 0.8995, val_acc = 0.6500\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2439, train_acc = 0.9004, val_acc = 0.6850\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2434, train_acc = 0.8997, val_acc = 0.6350\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2322, train_acc = 0.9000, val_acc = 0.6700\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2314, train_acc = 0.9017, val_acc = 0.6550\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2645, train_acc = 0.8961, val_acc = 0.6600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2579, train_acc = 0.8927, val_acc = 0.6150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2507, train_acc = 0.8944, val_acc = 0.6350\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2452, train_acc = 0.9012, val_acc = 0.6850\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2337, train_acc = 0.9029, val_acc = 0.6600\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2333, train_acc = 0.8997, val_acc = 0.6700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2698, train_acc = 0.9048, val_acc = 0.6800\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2740, train_acc = 0.9002, val_acc = 0.6900\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2664, train_acc = 0.9000, val_acc = 0.6700\n",
            "\n",
            "--------\n",
            "EPOCH 9\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.7358\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2923, train_acc = 0.8851, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2828, train_acc = 0.8905, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2646, train_acc = 0.8944, val_acc = 0.6200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2666, train_acc = 0.8946, val_acc = 0.6000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2606, train_acc = 0.8953, val_acc = 0.6100\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2490, train_acc = 0.8949, val_acc = 0.6050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2627, train_acc = 0.8958, val_acc = 0.6250\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2577, train_acc = 0.8970, val_acc = 0.6150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2468, train_acc = 0.9007, val_acc = 0.6350\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2587, train_acc = 0.8973, val_acc = 0.6450\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2532, train_acc = 0.8975, val_acc = 0.6300\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2478, train_acc = 0.9002, val_acc = 0.6350\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2412, train_acc = 0.8905, val_acc = 0.6400\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2380, train_acc = 0.8949, val_acc = 0.6300\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2436, train_acc = 0.8900, val_acc = 0.6100\n",
            "\n",
            "--------\n",
            "EPOCH 10\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.1288\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2685, train_acc = 0.8854, val_acc = 0.6200\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2682, train_acc = 0.8876, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2607, train_acc = 0.8861, val_acc = 0.6100\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2713, train_acc = 0.8927, val_acc = 0.5750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2744, train_acc = 0.8864, val_acc = 0.5950\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2604, train_acc = 0.8878, val_acc = 0.6000\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2530, train_acc = 0.8837, val_acc = 0.6050\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2535, train_acc = 0.8900, val_acc = 0.6000\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2413, train_acc = 0.8898, val_acc = 0.6050\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2841, train_acc = 0.8949, val_acc = 0.6150\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2722, train_acc = 0.8912, val_acc = 0.6100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2588, train_acc = 0.8966, val_acc = 0.6450\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2626, train_acc = 0.8907, val_acc = 0.6550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2587, train_acc = 0.8895, val_acc = 0.6200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2538, train_acc = 0.8883, val_acc = 0.6650\n",
            "\n",
            "--------\n",
            "EPOCH 11\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.3590\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2849, train_acc = 0.8810, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2786, train_acc = 0.8813, val_acc = 0.6400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2847, train_acc = 0.8747, val_acc = 0.6450\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2817, train_acc = 0.8779, val_acc = 0.6500\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2805, train_acc = 0.8779, val_acc = 0.6900\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2810, train_acc = 0.8759, val_acc = 0.6700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2947, train_acc = 0.8776, val_acc = 0.6800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2833, train_acc = 0.8776, val_acc = 0.6900\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2680, train_acc = 0.8757, val_acc = 0.6450\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2709, train_acc = 0.8851, val_acc = 0.6450\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2645, train_acc = 0.8919, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2622, train_acc = 0.8922, val_acc = 0.6900\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2872, train_acc = 0.8837, val_acc = 0.6550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2762, train_acc = 0.8839, val_acc = 0.6550\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2684, train_acc = 0.8844, val_acc = 0.6650\n",
            "\n",
            "--------\n",
            "EPOCH 12\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.1358\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3040, train_acc = 0.8711, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2976, train_acc = 0.8711, val_acc = 0.6300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2927, train_acc = 0.8749, val_acc = 0.6300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2948, train_acc = 0.8723, val_acc = 0.6450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2869, train_acc = 0.8749, val_acc = 0.6300\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2779, train_acc = 0.8771, val_acc = 0.6400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3082, train_acc = 0.8754, val_acc = 0.7050\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3164, train_acc = 0.8660, val_acc = 0.7050\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3004, train_acc = 0.8720, val_acc = 0.6850\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3011, train_acc = 0.8749, val_acc = 0.6400\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3007, train_acc = 0.8766, val_acc = 0.6100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2880, train_acc = 0.8800, val_acc = 0.6200\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3222, train_acc = 0.8766, val_acc = 0.6100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3097, train_acc = 0.8771, val_acc = 0.6300\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2974, train_acc = 0.8762, val_acc = 0.6400\n",
            "\n",
            "--------\n",
            "EPOCH 13\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.9687\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3407, train_acc = 0.8550, val_acc = 0.7250\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3230, train_acc = 0.8616, val_acc = 0.7300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3278, train_acc = 0.8638, val_acc = 0.7200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3351, train_acc = 0.8609, val_acc = 0.6850\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3207, train_acc = 0.8560, val_acc = 0.7150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3156, train_acc = 0.8638, val_acc = 0.7000\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3072, train_acc = 0.8677, val_acc = 0.6950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3024, train_acc = 0.8720, val_acc = 0.6850\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3055, train_acc = 0.8645, val_acc = 0.7000\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2985, train_acc = 0.8703, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2837, train_acc = 0.8703, val_acc = 0.7150\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2813, train_acc = 0.8701, val_acc = 0.7350\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2972, train_acc = 0.8745, val_acc = 0.6900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2936, train_acc = 0.8711, val_acc = 0.7200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2837, train_acc = 0.8732, val_acc = 0.7300\n",
            "\n",
            "--------\n",
            "EPOCH 14\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.3907\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3321, train_acc = 0.8672, val_acc = 0.6850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3258, train_acc = 0.8609, val_acc = 0.7350\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3205, train_acc = 0.8647, val_acc = 0.7000\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3192, train_acc = 0.8706, val_acc = 0.6950\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3162, train_acc = 0.8711, val_acc = 0.6800\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3019, train_acc = 0.8713, val_acc = 0.7500\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2851, train_acc = 0.8655, val_acc = 0.7900\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2817, train_acc = 0.8725, val_acc = 0.8150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2731, train_acc = 0.8720, val_acc = 0.8050\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2959, train_acc = 0.8674, val_acc = 0.7550\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2920, train_acc = 0.8677, val_acc = 0.7450\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2877, train_acc = 0.8696, val_acc = 0.7450\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3022, train_acc = 0.8725, val_acc = 0.8000\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3024, train_acc = 0.8696, val_acc = 0.7850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2879, train_acc = 0.8718, val_acc = 0.7900\n",
            "\n",
            "--------\n",
            "EPOCH 15\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0906\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3383, train_acc = 0.8701, val_acc = 0.7100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3374, train_acc = 0.8633, val_acc = 0.6950\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3227, train_acc = 0.8679, val_acc = 0.7100\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3230, train_acc = 0.8686, val_acc = 0.7750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3090, train_acc = 0.8660, val_acc = 0.7850\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2928, train_acc = 0.8698, val_acc = 0.7900\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2883, train_acc = 0.8715, val_acc = 0.7900\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2800, train_acc = 0.8715, val_acc = 0.7500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2814, train_acc = 0.8759, val_acc = 0.7650\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2947, train_acc = 0.8776, val_acc = 0.7450\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2946, train_acc = 0.8796, val_acc = 0.7550\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2850, train_acc = 0.8730, val_acc = 0.7150\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2899, train_acc = 0.8713, val_acc = 0.8100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2830, train_acc = 0.8779, val_acc = 0.8150\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2696, train_acc = 0.8834, val_acc = 0.8200\n",
            "\n",
            "--------\n",
            "EPOCH 16\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.1630\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2805, train_acc = 0.8803, val_acc = 0.7450\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2843, train_acc = 0.8723, val_acc = 0.7800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2764, train_acc = 0.8749, val_acc = 0.7500\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3006, train_acc = 0.8900, val_acc = 0.7200\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2956, train_acc = 0.8803, val_acc = 0.7350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2934, train_acc = 0.8791, val_acc = 0.7050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2700, train_acc = 0.8752, val_acc = 0.7800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2627, train_acc = 0.8781, val_acc = 0.7850\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2650, train_acc = 0.8803, val_acc = 0.7750\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2963, train_acc = 0.8817, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2918, train_acc = 0.8752, val_acc = 0.7250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2818, train_acc = 0.8779, val_acc = 0.7700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2914, train_acc = 0.8854, val_acc = 0.7650\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3002, train_acc = 0.8732, val_acc = 0.7550\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2806, train_acc = 0.8803, val_acc = 0.7600\n",
            "\n",
            "--------\n",
            "EPOCH 17\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0971\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3531, train_acc = 0.8672, val_acc = 0.7850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3250, train_acc = 0.8623, val_acc = 0.7650\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3159, train_acc = 0.8643, val_acc = 0.7800\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3018, train_acc = 0.8616, val_acc = 0.8200\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2838, train_acc = 0.8759, val_acc = 0.7900\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2888, train_acc = 0.8728, val_acc = 0.8150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3068, train_acc = 0.8796, val_acc = 0.8100\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3015, train_acc = 0.8786, val_acc = 0.8200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2877, train_acc = 0.8749, val_acc = 0.8100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2919, train_acc = 0.8752, val_acc = 0.8400\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2964, train_acc = 0.8723, val_acc = 0.8350\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2898, train_acc = 0.8718, val_acc = 0.8400\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2809, train_acc = 0.8757, val_acc = 0.8400\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2805, train_acc = 0.8737, val_acc = 0.8350\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2786, train_acc = 0.8759, val_acc = 0.8150\n",
            "\n",
            "--------\n",
            "EPOCH 18\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.9234\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3618, train_acc = 0.8601, val_acc = 0.7950\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3510, train_acc = 0.8584, val_acc = 0.8050\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3329, train_acc = 0.8715, val_acc = 0.7650\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3396, train_acc = 0.8618, val_acc = 0.7900\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3262, train_acc = 0.8613, val_acc = 0.7700\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3216, train_acc = 0.8662, val_acc = 0.7700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3041, train_acc = 0.8516, val_acc = 0.8150\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3064, train_acc = 0.8558, val_acc = 0.8400\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3047, train_acc = 0.8599, val_acc = 0.8250\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3064, train_acc = 0.8604, val_acc = 0.8050\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3016, train_acc = 0.8613, val_acc = 0.8250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2968, train_acc = 0.8628, val_acc = 0.8400\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3265, train_acc = 0.8647, val_acc = 0.7950\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3145, train_acc = 0.8618, val_acc = 0.8200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3092, train_acc = 0.8587, val_acc = 0.8150\n",
            "\n",
            "--------\n",
            "EPOCH 19\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.7381\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3538, train_acc = 0.8371, val_acc = 0.7850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3471, train_acc = 0.8402, val_acc = 0.7800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3355, train_acc = 0.8419, val_acc = 0.8200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3459, train_acc = 0.8456, val_acc = 0.8200\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3363, train_acc = 0.8453, val_acc = 0.8000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3267, train_acc = 0.8511, val_acc = 0.8050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3533, train_acc = 0.8439, val_acc = 0.7450\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3412, train_acc = 0.8470, val_acc = 0.7250\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3398, train_acc = 0.8465, val_acc = 0.7550\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3324, train_acc = 0.8507, val_acc = 0.7400\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3280, train_acc = 0.8511, val_acc = 0.7500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3203, train_acc = 0.8502, val_acc = 0.7250\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3324, train_acc = 0.8504, val_acc = 0.7200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3221, train_acc = 0.8545, val_acc = 0.7200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3239, train_acc = 0.8531, val_acc = 0.7450\n",
            "\n",
            "--------\n",
            "EPOCH 20\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8187\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3772, train_acc = 0.8298, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3594, train_acc = 0.8315, val_acc = 0.6750\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3656, train_acc = 0.8278, val_acc = 0.6500\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3886, train_acc = 0.8278, val_acc = 0.6700\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3770, train_acc = 0.8278, val_acc = 0.6600\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3706, train_acc = 0.8324, val_acc = 0.6600\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3539, train_acc = 0.8363, val_acc = 0.6600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3605, train_acc = 0.8315, val_acc = 0.6800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3452, train_acc = 0.8356, val_acc = 0.6700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3537, train_acc = 0.8366, val_acc = 0.7100\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3585, train_acc = 0.8400, val_acc = 0.6750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3387, train_acc = 0.8424, val_acc = 0.7100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3659, train_acc = 0.8363, val_acc = 0.6850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3723, train_acc = 0.8339, val_acc = 0.6450\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3668, train_acc = 0.8356, val_acc = 0.6900\n",
            "\n",
            "--------\n",
            "EPOCH 21\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.4761\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3925, train_acc = 0.8232, val_acc = 0.6000\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3658, train_acc = 0.8315, val_acc = 0.6100\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3639, train_acc = 0.8276, val_acc = 0.6350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3767, train_acc = 0.8286, val_acc = 0.6150\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3705, train_acc = 0.8295, val_acc = 0.5950\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3612, train_acc = 0.8276, val_acc = 0.5800\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3784, train_acc = 0.8256, val_acc = 0.5900\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3772, train_acc = 0.8198, val_acc = 0.5700\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3591, train_acc = 0.8271, val_acc = 0.5700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3596, train_acc = 0.8388, val_acc = 0.5550\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3577, train_acc = 0.8339, val_acc = 0.5600\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3534, train_acc = 0.8390, val_acc = 0.5750\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3869, train_acc = 0.8339, val_acc = 0.5600\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3790, train_acc = 0.8344, val_acc = 0.5550\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3738, train_acc = 0.8397, val_acc = 0.5550\n",
            "\n",
            "--------\n",
            "EPOCH 22\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.4029\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3973, train_acc = 0.8222, val_acc = 0.5750\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3896, train_acc = 0.8203, val_acc = 0.6000\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3792, train_acc = 0.8237, val_acc = 0.5900\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3885, train_acc = 0.8169, val_acc = 0.5850\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3744, train_acc = 0.8259, val_acc = 0.5650\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3662, train_acc = 0.8264, val_acc = 0.6050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3710, train_acc = 0.8293, val_acc = 0.5700\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3720, train_acc = 0.8293, val_acc = 0.5700\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3577, train_acc = 0.8329, val_acc = 0.6050\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3532, train_acc = 0.8354, val_acc = 0.5700\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3468, train_acc = 0.8344, val_acc = 0.6100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3437, train_acc = 0.8400, val_acc = 0.5650\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3898, train_acc = 0.8378, val_acc = 0.5600\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3726, train_acc = 0.8405, val_acc = 0.5400\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3654, train_acc = 0.8417, val_acc = 0.5650\n",
            "\n",
            "--------\n",
            "EPOCH 23\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.2746\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3768, train_acc = 0.8315, val_acc = 0.5650\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3560, train_acc = 0.8320, val_acc = 0.5850\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3511, train_acc = 0.8358, val_acc = 0.5750\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3707, train_acc = 0.8375, val_acc = 0.6050\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3559, train_acc = 0.8409, val_acc = 0.5750\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3473, train_acc = 0.8388, val_acc = 0.5400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3596, train_acc = 0.8414, val_acc = 0.5800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3399, train_acc = 0.8502, val_acc = 0.5950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3406, train_acc = 0.8453, val_acc = 0.5600\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3572, train_acc = 0.8422, val_acc = 0.5600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3396, train_acc = 0.8514, val_acc = 0.5950\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.3394, train_acc = 0.8429, val_acc = 0.5750\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3458, train_acc = 0.8550, val_acc = 0.6200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3436, train_acc = 0.8477, val_acc = 0.6050\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3291, train_acc = 0.8514, val_acc = 0.5950\n",
            "\n",
            "--------\n",
            "EPOCH 24\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.8954\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3369, train_acc = 0.8550, val_acc = 0.6200\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3323, train_acc = 0.8475, val_acc = 0.6050\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.3326, train_acc = 0.8545, val_acc = 0.6050\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3277, train_acc = 0.8618, val_acc = 0.5950\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.3219, train_acc = 0.8543, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.3178, train_acc = 0.8594, val_acc = 0.6150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.3313, train_acc = 0.8577, val_acc = 0.6200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.3222, train_acc = 0.8655, val_acc = 0.6700\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.3250, train_acc = 0.8575, val_acc = 0.6200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2977, train_acc = 0.8689, val_acc = 0.6100\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.3060, train_acc = 0.8689, val_acc = 0.5950\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2873, train_acc = 0.8766, val_acc = 0.5900\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3109, train_acc = 0.8679, val_acc = 0.5850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.3122, train_acc = 0.8587, val_acc = 0.5850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.3145, train_acc = 0.8633, val_acc = 0.5900\n",
            "\n",
            "--------\n",
            "EPOCH 25\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.7730\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.3193, train_acc = 0.8647, val_acc = 0.5800\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.3154, train_acc = 0.8669, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2962, train_acc = 0.8737, val_acc = 0.5750\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2880, train_acc = 0.8737, val_acc = 0.5750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2864, train_acc = 0.8749, val_acc = 0.5850\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2756, train_acc = 0.8820, val_acc = 0.5650\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2795, train_acc = 0.8798, val_acc = 0.5950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2771, train_acc = 0.8742, val_acc = 0.5350\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2754, train_acc = 0.8764, val_acc = 0.5700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2930, train_acc = 0.8844, val_acc = 0.5950\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2996, train_acc = 0.8752, val_acc = 0.5800\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2922, train_acc = 0.8762, val_acc = 0.5650\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2882, train_acc = 0.8830, val_acc = 0.5500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2740, train_acc = 0.8817, val_acc = 0.5500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2686, train_acc = 0.8910, val_acc = 0.5450\n",
            "\n",
            "--------\n",
            "EPOCH 26\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.0166\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2971, train_acc = 0.8813, val_acc = 0.5550\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2894, train_acc = 0.8808, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2696, train_acc = 0.8849, val_acc = 0.5450\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2848, train_acc = 0.8781, val_acc = 0.6050\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2783, train_acc = 0.8837, val_acc = 0.6300\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2701, train_acc = 0.8849, val_acc = 0.6400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2631, train_acc = 0.8851, val_acc = 0.6700\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2538, train_acc = 0.8956, val_acc = 0.6200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2621, train_acc = 0.8929, val_acc = 0.6200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2587, train_acc = 0.8895, val_acc = 0.6750\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2629, train_acc = 0.8898, val_acc = 0.6650\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2612, train_acc = 0.8922, val_acc = 0.6800\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2501, train_acc = 0.8907, val_acc = 0.6400\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2377, train_acc = 0.8951, val_acc = 0.6000\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2357, train_acc = 0.9031, val_acc = 0.6350\n",
            "\n",
            "--------\n",
            "EPOCH 27\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.5507\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2772, train_acc = 0.8822, val_acc = 0.5500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2708, train_acc = 0.8861, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2593, train_acc = 0.8864, val_acc = 0.5600\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2741, train_acc = 0.8868, val_acc = 0.5600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2852, train_acc = 0.8871, val_acc = 0.5250\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2774, train_acc = 0.8907, val_acc = 0.5400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2479, train_acc = 0.8953, val_acc = 0.6000\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2441, train_acc = 0.8915, val_acc = 0.5950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2455, train_acc = 0.8992, val_acc = 0.5850\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2555, train_acc = 0.8912, val_acc = 0.6050\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2534, train_acc = 0.8936, val_acc = 0.5750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2458, train_acc = 0.8963, val_acc = 0.5500\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2728, train_acc = 0.8866, val_acc = 0.5500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2614, train_acc = 0.8895, val_acc = 0.5750\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2623, train_acc = 0.8803, val_acc = 0.5700\n",
            "\n",
            "--------\n",
            "EPOCH 28\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.8905\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2560, train_acc = 0.8885, val_acc = 0.6350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2463, train_acc = 0.8856, val_acc = 0.6450\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2509, train_acc = 0.8866, val_acc = 0.6150\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2654, train_acc = 0.8815, val_acc = 0.6000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2539, train_acc = 0.8859, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2490, train_acc = 0.8864, val_acc = 0.6150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2625, train_acc = 0.8856, val_acc = 0.5800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2535, train_acc = 0.8859, val_acc = 0.5700\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2542, train_acc = 0.8905, val_acc = 0.5700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2792, train_acc = 0.8837, val_acc = 0.5700\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2645, train_acc = 0.8830, val_acc = 0.5900\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2638, train_acc = 0.8895, val_acc = 0.5700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2729, train_acc = 0.8890, val_acc = 0.6050\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2540, train_acc = 0.8883, val_acc = 0.5950\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2556, train_acc = 0.8893, val_acc = 0.5600\n",
            "\n",
            "--------\n",
            "EPOCH 29\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.1458\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2990, train_acc = 0.8815, val_acc = 0.5750\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2793, train_acc = 0.8798, val_acc = 0.5450\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2833, train_acc = 0.8774, val_acc = 0.5850\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2643, train_acc = 0.8868, val_acc = 0.5750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2613, train_acc = 0.8844, val_acc = 0.5750\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2570, train_acc = 0.8910, val_acc = 0.5750\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2566, train_acc = 0.8873, val_acc = 0.5550\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2403, train_acc = 0.8905, val_acc = 0.5500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2423, train_acc = 0.8890, val_acc = 0.5600\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2755, train_acc = 0.8873, val_acc = 0.5500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2764, train_acc = 0.8861, val_acc = 0.5700\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2683, train_acc = 0.8849, val_acc = 0.5950\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.3127, train_acc = 0.8834, val_acc = 0.5800\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2910, train_acc = 0.8749, val_acc = 0.5700\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2816, train_acc = 0.8834, val_acc = 0.5600\n",
            "\n",
            "--------\n",
            "EPOCH 30\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.7488\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2840, train_acc = 0.8849, val_acc = 0.5800\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2799, train_acc = 0.8793, val_acc = 0.6050\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2645, train_acc = 0.8817, val_acc = 0.5850\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.3051, train_acc = 0.8832, val_acc = 0.6250\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2924, train_acc = 0.8718, val_acc = 0.6500\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2849, train_acc = 0.8776, val_acc = 0.6450\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2980, train_acc = 0.8861, val_acc = 0.6150\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2883, train_acc = 0.8820, val_acc = 0.6550\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2820, train_acc = 0.8815, val_acc = 0.6100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2695, train_acc = 0.8895, val_acc = 0.6100\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2742, train_acc = 0.8832, val_acc = 0.6250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2568, train_acc = 0.8919, val_acc = 0.6100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2526, train_acc = 0.8876, val_acc = 0.6550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2393, train_acc = 0.8917, val_acc = 0.6650\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2414, train_acc = 0.8922, val_acc = 0.6400\n",
            "\n",
            "--------\n",
            "EPOCH 31\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.0952\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2700, train_acc = 0.8900, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2659, train_acc = 0.8803, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2676, train_acc = 0.8878, val_acc = 0.5950\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2833, train_acc = 0.8788, val_acc = 0.5800\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2666, train_acc = 0.8861, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2584, train_acc = 0.8895, val_acc = 0.5550\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2546, train_acc = 0.8883, val_acc = 0.5950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2590, train_acc = 0.8876, val_acc = 0.5800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2654, train_acc = 0.8810, val_acc = 0.5550\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2690, train_acc = 0.8871, val_acc = 0.5800\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2587, train_acc = 0.8924, val_acc = 0.5750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2534, train_acc = 0.8917, val_acc = 0.5900\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2816, train_acc = 0.8851, val_acc = 0.5300\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2708, train_acc = 0.8893, val_acc = 0.5850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2496, train_acc = 0.8939, val_acc = 0.5650\n",
            "\n",
            "--------\n",
            "EPOCH 32\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.2172\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2797, train_acc = 0.8895, val_acc = 0.5850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2656, train_acc = 0.8927, val_acc = 0.5850\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2535, train_acc = 0.8927, val_acc = 0.6000\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2688, train_acc = 0.8849, val_acc = 0.6100\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2656, train_acc = 0.8851, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2520, train_acc = 0.8893, val_acc = 0.6150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2562, train_acc = 0.8847, val_acc = 0.6200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2465, train_acc = 0.8912, val_acc = 0.6150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2480, train_acc = 0.8910, val_acc = 0.6100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2749, train_acc = 0.8888, val_acc = 0.6000\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2642, train_acc = 0.8944, val_acc = 0.6250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2583, train_acc = 0.8924, val_acc = 0.6150\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2922, train_acc = 0.8917, val_acc = 0.5850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2771, train_acc = 0.8898, val_acc = 0.5800\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2810, train_acc = 0.8902, val_acc = 0.5650\n",
            "\n",
            "--------\n",
            "EPOCH 33\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.4765\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2672, train_acc = 0.8929, val_acc = 0.5850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2698, train_acc = 0.8966, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2535, train_acc = 0.8927, val_acc = 0.5900\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2421, train_acc = 0.8915, val_acc = 0.6000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2376, train_acc = 0.8939, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2314, train_acc = 0.8958, val_acc = 0.5850\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2530, train_acc = 0.8941, val_acc = 0.5700\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2557, train_acc = 0.8941, val_acc = 0.5950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2578, train_acc = 0.8900, val_acc = 0.5800\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2557, train_acc = 0.8890, val_acc = 0.6150\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2454, train_acc = 0.8927, val_acc = 0.5950\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2437, train_acc = 0.8941, val_acc = 0.6050\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2406, train_acc = 0.8956, val_acc = 0.6100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2418, train_acc = 0.8963, val_acc = 0.6500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2313, train_acc = 0.9002, val_acc = 0.6150\n",
            "\n",
            "--------\n",
            "EPOCH 34\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5188\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2922, train_acc = 0.8932, val_acc = 0.5850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2699, train_acc = 0.8958, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2617, train_acc = 0.8912, val_acc = 0.6150\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2675, train_acc = 0.8966, val_acc = 0.6450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2711, train_acc = 0.9031, val_acc = 0.6150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2673, train_acc = 0.8953, val_acc = 0.6250\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2447, train_acc = 0.8997, val_acc = 0.6400\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2295, train_acc = 0.9009, val_acc = 0.6650\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2273, train_acc = 0.9036, val_acc = 0.6050\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2584, train_acc = 0.9038, val_acc = 0.6600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2499, train_acc = 0.8995, val_acc = 0.6500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2502, train_acc = 0.8985, val_acc = 0.6450\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2454, train_acc = 0.8956, val_acc = 0.6200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2410, train_acc = 0.8956, val_acc = 0.6250\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2357, train_acc = 0.8970, val_acc = 0.6100\n",
            "\n",
            "--------\n",
            "EPOCH 35\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.4256\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2524, train_acc = 0.8949, val_acc = 0.6400\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2467, train_acc = 0.8924, val_acc = 0.6400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2410, train_acc = 0.8927, val_acc = 0.6450\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2288, train_acc = 0.8963, val_acc = 0.6150\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2173, train_acc = 0.9065, val_acc = 0.6350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2181, train_acc = 0.9046, val_acc = 0.6350\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2595, train_acc = 0.9002, val_acc = 0.6200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2504, train_acc = 0.9014, val_acc = 0.6350\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2408, train_acc = 0.9029, val_acc = 0.6200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2359, train_acc = 0.9007, val_acc = 0.6050\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2316, train_acc = 0.9065, val_acc = 0.6050\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2387, train_acc = 0.9021, val_acc = 0.6000\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2001, train_acc = 0.9114, val_acc = 0.6350\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2011, train_acc = 0.9116, val_acc = 0.6500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1958, train_acc = 0.9133, val_acc = 0.6600\n",
            "\n",
            "--------\n",
            "EPOCH 36\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6136\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2400, train_acc = 0.9026, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2459, train_acc = 0.9019, val_acc = 0.6000\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2290, train_acc = 0.9053, val_acc = 0.6250\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2290, train_acc = 0.9019, val_acc = 0.6400\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2223, train_acc = 0.9043, val_acc = 0.6450\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2134, train_acc = 0.9150, val_acc = 0.6300\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2277, train_acc = 0.9058, val_acc = 0.7200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2235, train_acc = 0.9051, val_acc = 0.7300\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2211, train_acc = 0.9029, val_acc = 0.7000\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2245, train_acc = 0.9065, val_acc = 0.6850\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2210, train_acc = 0.9058, val_acc = 0.6500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2111, train_acc = 0.9082, val_acc = 0.6700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2171, train_acc = 0.9143, val_acc = 0.6900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2100, train_acc = 0.9143, val_acc = 0.6750\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2142, train_acc = 0.9094, val_acc = 0.7050\n",
            "\n",
            "--------\n",
            "EPOCH 37\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8347\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2263, train_acc = 0.9094, val_acc = 0.7400\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2242, train_acc = 0.9116, val_acc = 0.7300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2237, train_acc = 0.9123, val_acc = 0.7100\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2122, train_acc = 0.9157, val_acc = 0.7000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2109, train_acc = 0.9051, val_acc = 0.7300\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2105, train_acc = 0.9109, val_acc = 0.7150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2132, train_acc = 0.9109, val_acc = 0.6950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2092, train_acc = 0.9119, val_acc = 0.6550\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2092, train_acc = 0.9123, val_acc = 0.6950\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2097, train_acc = 0.9104, val_acc = 0.6750\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2034, train_acc = 0.9148, val_acc = 0.7050\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2034, train_acc = 0.9102, val_acc = 0.7350\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2200, train_acc = 0.9034, val_acc = 0.7500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2138, train_acc = 0.9060, val_acc = 0.7500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2145, train_acc = 0.9094, val_acc = 0.7300\n",
            "\n",
            "--------\n",
            "EPOCH 38\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0686\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2465, train_acc = 0.9048, val_acc = 0.6550\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2281, train_acc = 0.9068, val_acc = 0.6650\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2233, train_acc = 0.9063, val_acc = 0.6700\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2029, train_acc = 0.9097, val_acc = 0.6800\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2016, train_acc = 0.9104, val_acc = 0.6850\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1989, train_acc = 0.9148, val_acc = 0.6850\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2133, train_acc = 0.9133, val_acc = 0.6350\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2261, train_acc = 0.9114, val_acc = 0.6550\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2274, train_acc = 0.9085, val_acc = 0.6700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2290, train_acc = 0.9119, val_acc = 0.7300\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2219, train_acc = 0.9157, val_acc = 0.7400\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2068, train_acc = 0.9169, val_acc = 0.7200\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2164, train_acc = 0.9075, val_acc = 0.7200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2083, train_acc = 0.9087, val_acc = 0.7200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2090, train_acc = 0.9145, val_acc = 0.7550\n",
            "\n",
            "--------\n",
            "EPOCH 39\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0693\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2276, train_acc = 0.9080, val_acc = 0.7500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2267, train_acc = 0.9077, val_acc = 0.7400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2262, train_acc = 0.9087, val_acc = 0.7400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2151, train_acc = 0.9041, val_acc = 0.7800\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2024, train_acc = 0.9155, val_acc = 0.7450\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2095, train_acc = 0.9070, val_acc = 0.7500\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2484, train_acc = 0.9072, val_acc = 0.7600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2403, train_acc = 0.9109, val_acc = 0.7250\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2233, train_acc = 0.9097, val_acc = 0.7200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2040, train_acc = 0.9089, val_acc = 0.7550\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2005, train_acc = 0.9150, val_acc = 0.7750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1994, train_acc = 0.9116, val_acc = 0.7250\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2160, train_acc = 0.9123, val_acc = 0.7500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2195, train_acc = 0.9145, val_acc = 0.7550\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2071, train_acc = 0.9153, val_acc = 0.7600\n",
            "\n",
            "--------\n",
            "EPOCH 40\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.9952\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2159, train_acc = 0.9063, val_acc = 0.7200\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2090, train_acc = 0.9082, val_acc = 0.7500\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2016, train_acc = 0.9119, val_acc = 0.7350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2245, train_acc = 0.9051, val_acc = 0.7250\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2208, train_acc = 0.9038, val_acc = 0.7350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2200, train_acc = 0.9046, val_acc = 0.7200\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2541, train_acc = 0.9004, val_acc = 0.7450\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2235, train_acc = 0.9116, val_acc = 0.7150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2279, train_acc = 0.9060, val_acc = 0.7350\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2307, train_acc = 0.9080, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2297, train_acc = 0.9041, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2132, train_acc = 0.9099, val_acc = 0.7250\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2124, train_acc = 0.9116, val_acc = 0.7500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2030, train_acc = 0.9150, val_acc = 0.7100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2064, train_acc = 0.9131, val_acc = 0.7200\n",
            "\n",
            "--------\n",
            "EPOCH 41\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.2372\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2218, train_acc = 0.9007, val_acc = 0.7100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2184, train_acc = 0.9019, val_acc = 0.7250\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2145, train_acc = 0.9038, val_acc = 0.7400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2194, train_acc = 0.8987, val_acc = 0.7500\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2132, train_acc = 0.9029, val_acc = 0.7350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2108, train_acc = 0.9046, val_acc = 0.7700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2239, train_acc = 0.9058, val_acc = 0.7500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2269, train_acc = 0.9048, val_acc = 0.7600\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2157, train_acc = 0.9068, val_acc = 0.7500\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2206, train_acc = 0.9060, val_acc = 0.7500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2029, train_acc = 0.9128, val_acc = 0.7350\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2111, train_acc = 0.9111, val_acc = 0.7300\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2244, train_acc = 0.9043, val_acc = 0.7600\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2159, train_acc = 0.9048, val_acc = 0.7450\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2171, train_acc = 0.9077, val_acc = 0.7800\n",
            "\n",
            "--------\n",
            "EPOCH 42\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0879\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2145, train_acc = 0.9026, val_acc = 0.7400\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2117, train_acc = 0.9089, val_acc = 0.7450\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2000, train_acc = 0.9089, val_acc = 0.7200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2182, train_acc = 0.9051, val_acc = 0.7300\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2094, train_acc = 0.9070, val_acc = 0.7350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2087, train_acc = 0.9114, val_acc = 0.7700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2374, train_acc = 0.9021, val_acc = 0.6800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2306, train_acc = 0.9031, val_acc = 0.7200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2389, train_acc = 0.9019, val_acc = 0.7100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2337, train_acc = 0.9063, val_acc = 0.7300\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2375, train_acc = 0.9036, val_acc = 0.7200\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2351, train_acc = 0.8973, val_acc = 0.7350\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2279, train_acc = 0.9080, val_acc = 0.7650\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2246, train_acc = 0.9077, val_acc = 0.7600\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2229, train_acc = 0.9075, val_acc = 0.7650\n",
            "\n",
            "--------\n",
            "EPOCH 43\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.1246\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2317, train_acc = 0.8997, val_acc = 0.7200\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2246, train_acc = 0.8983, val_acc = 0.7250\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2199, train_acc = 0.9009, val_acc = 0.7150\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2367, train_acc = 0.8968, val_acc = 0.6900\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2299, train_acc = 0.8980, val_acc = 0.6850\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2292, train_acc = 0.8970, val_acc = 0.6550\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2346, train_acc = 0.8990, val_acc = 0.7050\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2507, train_acc = 0.8941, val_acc = 0.7450\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2343, train_acc = 0.8941, val_acc = 0.7250\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2371, train_acc = 0.8973, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2226, train_acc = 0.8983, val_acc = 0.7150\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2302, train_acc = 0.9002, val_acc = 0.7200\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2694, train_acc = 0.9024, val_acc = 0.6850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2395, train_acc = 0.8980, val_acc = 0.7100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2523, train_acc = 0.9002, val_acc = 0.7150\n",
            "\n",
            "--------\n",
            "EPOCH 44\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.2532\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2411, train_acc = 0.8929, val_acc = 0.7750\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2344, train_acc = 0.8941, val_acc = 0.7400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2340, train_acc = 0.8980, val_acc = 0.7350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2511, train_acc = 0.8932, val_acc = 0.7400\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2388, train_acc = 0.8900, val_acc = 0.7400\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2456, train_acc = 0.8973, val_acc = 0.7100\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2640, train_acc = 0.8929, val_acc = 0.7600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2487, train_acc = 0.8968, val_acc = 0.7350\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2337, train_acc = 0.8932, val_acc = 0.7300\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2477, train_acc = 0.8900, val_acc = 0.7500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2508, train_acc = 0.8876, val_acc = 0.7300\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2481, train_acc = 0.8883, val_acc = 0.7400\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2339, train_acc = 0.8973, val_acc = 0.7550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2337, train_acc = 0.8953, val_acc = 0.7850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2268, train_acc = 0.9031, val_acc = 0.7550\n",
            "\n",
            "--------\n",
            "EPOCH 45\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.3524\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2893, train_acc = 0.8871, val_acc = 0.6650\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2869, train_acc = 0.8837, val_acc = 0.7050\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2767, train_acc = 0.8915, val_acc = 0.6850\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2705, train_acc = 0.8861, val_acc = 0.6600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2582, train_acc = 0.8849, val_acc = 0.7050\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2386, train_acc = 0.8958, val_acc = 0.6700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2682, train_acc = 0.8924, val_acc = 0.6600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2632, train_acc = 0.8898, val_acc = 0.6550\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2585, train_acc = 0.8868, val_acc = 0.6600\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2686, train_acc = 0.8851, val_acc = 0.6700\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2535, train_acc = 0.8883, val_acc = 0.6450\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2523, train_acc = 0.8898, val_acc = 0.6550\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2584, train_acc = 0.8917, val_acc = 0.6950\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2535, train_acc = 0.8900, val_acc = 0.6750\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2455, train_acc = 0.8939, val_acc = 0.6800\n",
            "\n",
            "--------\n",
            "EPOCH 46\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.4742\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2632, train_acc = 0.8895, val_acc = 0.6600\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2543, train_acc = 0.8873, val_acc = 0.6350\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2780, train_acc = 0.8871, val_acc = 0.6350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2601, train_acc = 0.8861, val_acc = 0.6800\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2541, train_acc = 0.8854, val_acc = 0.6650\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2527, train_acc = 0.8885, val_acc = 0.7100\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2667, train_acc = 0.8815, val_acc = 0.7000\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2651, train_acc = 0.8842, val_acc = 0.7100\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2469, train_acc = 0.8847, val_acc = 0.7400\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2575, train_acc = 0.8902, val_acc = 0.7350\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2535, train_acc = 0.8922, val_acc = 0.7000\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2520, train_acc = 0.8864, val_acc = 0.6850\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2585, train_acc = 0.8788, val_acc = 0.7100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2570, train_acc = 0.8881, val_acc = 0.6950\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2455, train_acc = 0.8873, val_acc = 0.7200\n",
            "\n",
            "--------\n",
            "EPOCH 47\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.4228\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2803, train_acc = 0.8732, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2749, train_acc = 0.8749, val_acc = 0.6300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2637, train_acc = 0.8728, val_acc = 0.6600\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2685, train_acc = 0.8827, val_acc = 0.6750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2576, train_acc = 0.8783, val_acc = 0.6650\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2499, train_acc = 0.8866, val_acc = 0.6800\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2706, train_acc = 0.8924, val_acc = 0.6800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2687, train_acc = 0.8839, val_acc = 0.6850\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2651, train_acc = 0.8864, val_acc = 0.6900\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.3003, train_acc = 0.8885, val_acc = 0.6500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2884, train_acc = 0.8902, val_acc = 0.6450\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2690, train_acc = 0.8888, val_acc = 0.6550\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2511, train_acc = 0.8825, val_acc = 0.6500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2402, train_acc = 0.8917, val_acc = 0.7100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2411, train_acc = 0.8881, val_acc = 0.6450\n",
            "\n",
            "--------\n",
            "EPOCH 48\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.4022\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2804, train_acc = 0.8791, val_acc = 0.6450\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2688, train_acc = 0.8815, val_acc = 0.6600\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2641, train_acc = 0.8839, val_acc = 0.6250\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2929, train_acc = 0.8803, val_acc = 0.5750\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2786, train_acc = 0.8757, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2784, train_acc = 0.8774, val_acc = 0.5850\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2769, train_acc = 0.8830, val_acc = 0.6200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2582, train_acc = 0.8866, val_acc = 0.6150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2507, train_acc = 0.8861, val_acc = 0.6300\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2611, train_acc = 0.8895, val_acc = 0.6300\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2656, train_acc = 0.8888, val_acc = 0.6100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2557, train_acc = 0.8873, val_acc = 0.6200\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2507, train_acc = 0.8890, val_acc = 0.6200\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2513, train_acc = 0.8786, val_acc = 0.6100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2431, train_acc = 0.8878, val_acc = 0.6250\n",
            "\n",
            "--------\n",
            "EPOCH 49\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.9921\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2743, train_acc = 0.8805, val_acc = 0.7350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2670, train_acc = 0.8791, val_acc = 0.7350\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2786, train_acc = 0.8815, val_acc = 0.7750\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2464, train_acc = 0.8929, val_acc = 0.7150\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2346, train_acc = 0.8876, val_acc = 0.7150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2316, train_acc = 0.8936, val_acc = 0.7100\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2447, train_acc = 0.8893, val_acc = 0.7500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2378, train_acc = 0.8944, val_acc = 0.7200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2358, train_acc = 0.8915, val_acc = 0.7200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2540, train_acc = 0.8927, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2520, train_acc = 0.8936, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2435, train_acc = 0.8929, val_acc = 0.7250\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2489, train_acc = 0.8939, val_acc = 0.7400\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2399, train_acc = 0.8951, val_acc = 0.7350\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2400, train_acc = 0.8893, val_acc = 0.7250\n",
            "\n",
            "--------\n",
            "EPOCH 50\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.0270\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2356, train_acc = 0.8997, val_acc = 0.6700\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2302, train_acc = 0.8953, val_acc = 0.6750\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2228, train_acc = 0.9026, val_acc = 0.7050\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2601, train_acc = 0.8980, val_acc = 0.7150\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2377, train_acc = 0.9041, val_acc = 0.7100\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2324, train_acc = 0.9031, val_acc = 0.6750\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2481, train_acc = 0.8941, val_acc = 0.6800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2454, train_acc = 0.8970, val_acc = 0.7200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2294, train_acc = 0.9019, val_acc = 0.6700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2275, train_acc = 0.8995, val_acc = 0.7550\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2140, train_acc = 0.9102, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2144, train_acc = 0.9065, val_acc = 0.7500\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2213, train_acc = 0.9031, val_acc = 0.7350\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2184, train_acc = 0.9048, val_acc = 0.7500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2120, train_acc = 0.9024, val_acc = 0.7300\n",
            "\n",
            "--------\n",
            "EPOCH 51\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.1659\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2180, train_acc = 0.9072, val_acc = 0.7300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2188, train_acc = 0.9060, val_acc = 0.7400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2112, train_acc = 0.9111, val_acc = 0.7400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2255, train_acc = 0.9036, val_acc = 0.7200\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2183, train_acc = 0.9065, val_acc = 0.7150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2043, train_acc = 0.9106, val_acc = 0.7050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2234, train_acc = 0.9116, val_acc = 0.7150\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2109, train_acc = 0.9092, val_acc = 0.7150\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2007, train_acc = 0.9089, val_acc = 0.7250\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1927, train_acc = 0.9167, val_acc = 0.6800\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.1906, train_acc = 0.9162, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1798, train_acc = 0.9216, val_acc = 0.7050\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2131, train_acc = 0.9116, val_acc = 0.7100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2112, train_acc = 0.9087, val_acc = 0.6900\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2013, train_acc = 0.9153, val_acc = 0.7200\n",
            "\n",
            "--------\n",
            "EPOCH 52\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 11.2572\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2076, train_acc = 0.9082, val_acc = 0.7350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2055, train_acc = 0.9102, val_acc = 0.7050\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.1993, train_acc = 0.9119, val_acc = 0.7200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2099, train_acc = 0.9143, val_acc = 0.7250\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2107, train_acc = 0.9138, val_acc = 0.7100\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1949, train_acc = 0.9160, val_acc = 0.7350\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2016, train_acc = 0.9172, val_acc = 0.7250\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.1858, train_acc = 0.9182, val_acc = 0.7550\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1824, train_acc = 0.9196, val_acc = 0.7100\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1930, train_acc = 0.9143, val_acc = 0.7250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2012, train_acc = 0.9203, val_acc = 0.7500\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1902, train_acc = 0.9179, val_acc = 0.7400\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1997, train_acc = 0.9148, val_acc = 0.7900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1957, train_acc = 0.9153, val_acc = 0.7600\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1859, train_acc = 0.9155, val_acc = 0.7750\n",
            "\n",
            "--------\n",
            "EPOCH 53\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.7104\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2145, train_acc = 0.9121, val_acc = 0.7350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2147, train_acc = 0.9092, val_acc = 0.7450\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2058, train_acc = 0.9157, val_acc = 0.7450\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2093, train_acc = 0.9126, val_acc = 0.7450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.1990, train_acc = 0.9206, val_acc = 0.7050\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2029, train_acc = 0.9177, val_acc = 0.6850\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.1991, train_acc = 0.9157, val_acc = 0.7200\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2099, train_acc = 0.9140, val_acc = 0.7500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1889, train_acc = 0.9148, val_acc = 0.7250\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2183, train_acc = 0.9111, val_acc = 0.7500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2197, train_acc = 0.9148, val_acc = 0.7250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1993, train_acc = 0.9138, val_acc = 0.7400\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1925, train_acc = 0.9237, val_acc = 0.7000\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1899, train_acc = 0.9228, val_acc = 0.7400\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1897, train_acc = 0.9165, val_acc = 0.7150\n",
            "\n",
            "--------\n",
            "EPOCH 54\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8385\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2051, train_acc = 0.9121, val_acc = 0.7350\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2000, train_acc = 0.9119, val_acc = 0.7250\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.1976, train_acc = 0.9140, val_acc = 0.7000\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2232, train_acc = 0.9102, val_acc = 0.7300\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2188, train_acc = 0.9104, val_acc = 0.7150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2032, train_acc = 0.9082, val_acc = 0.7150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.1986, train_acc = 0.9201, val_acc = 0.6950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.1965, train_acc = 0.9128, val_acc = 0.7250\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1892, train_acc = 0.9169, val_acc = 0.7000\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1970, train_acc = 0.9109, val_acc = 0.6600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2070, train_acc = 0.9089, val_acc = 0.6650\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1980, train_acc = 0.9114, val_acc = 0.6750\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2019, train_acc = 0.9140, val_acc = 0.6950\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1981, train_acc = 0.9131, val_acc = 0.6750\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1962, train_acc = 0.9136, val_acc = 0.7050\n",
            "\n",
            "--------\n",
            "EPOCH 55\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.8130\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2223, train_acc = 0.8966, val_acc = 0.6650\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2179, train_acc = 0.8968, val_acc = 0.6800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2164, train_acc = 0.9007, val_acc = 0.6550\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2258, train_acc = 0.9085, val_acc = 0.6450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2296, train_acc = 0.9034, val_acc = 0.6750\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2242, train_acc = 0.9075, val_acc = 0.6800\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2185, train_acc = 0.9070, val_acc = 0.6500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2088, train_acc = 0.9080, val_acc = 0.6900\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2118, train_acc = 0.9082, val_acc = 0.6800\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2231, train_acc = 0.9029, val_acc = 0.6800\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2236, train_acc = 0.9055, val_acc = 0.6650\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2188, train_acc = 0.9082, val_acc = 0.6700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2139, train_acc = 0.9089, val_acc = 0.6250\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2096, train_acc = 0.9041, val_acc = 0.6400\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2069, train_acc = 0.9089, val_acc = 0.6650\n",
            "\n",
            "--------\n",
            "EPOCH 56\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.7185\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2422, train_acc = 0.9004, val_acc = 0.5500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2354, train_acc = 0.9036, val_acc = 0.5450\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2234, train_acc = 0.9012, val_acc = 0.5600\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2877, train_acc = 0.9029, val_acc = 0.5600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2498, train_acc = 0.8934, val_acc = 0.5350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2294, train_acc = 0.8963, val_acc = 0.5700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2312, train_acc = 0.8966, val_acc = 0.5950\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2341, train_acc = 0.8992, val_acc = 0.6050\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2284, train_acc = 0.9017, val_acc = 0.5600\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2453, train_acc = 0.9021, val_acc = 0.5700\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2359, train_acc = 0.9048, val_acc = 0.5900\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2324, train_acc = 0.8987, val_acc = 0.5300\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2355, train_acc = 0.9133, val_acc = 0.6300\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2266, train_acc = 0.9126, val_acc = 0.6200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2130, train_acc = 0.9123, val_acc = 0.6000\n",
            "\n",
            "--------\n",
            "EPOCH 57\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6087\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2183, train_acc = 0.8997, val_acc = 0.6500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2104, train_acc = 0.9075, val_acc = 0.6550\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2191, train_acc = 0.9019, val_acc = 0.6500\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2541, train_acc = 0.9017, val_acc = 0.6450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2431, train_acc = 0.8975, val_acc = 0.6250\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2431, train_acc = 0.8966, val_acc = 0.6700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2347, train_acc = 0.9029, val_acc = 0.6650\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2241, train_acc = 0.9036, val_acc = 0.6700\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2260, train_acc = 0.9087, val_acc = 0.6700\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2306, train_acc = 0.9014, val_acc = 0.6500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2349, train_acc = 0.8997, val_acc = 0.6950\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2362, train_acc = 0.9000, val_acc = 0.6800\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2124, train_acc = 0.9029, val_acc = 0.6550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2158, train_acc = 0.9058, val_acc = 0.6850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2121, train_acc = 0.9104, val_acc = 0.6850\n",
            "\n",
            "--------\n",
            "EPOCH 58\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6014\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2432, train_acc = 0.9014, val_acc = 0.6250\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2349, train_acc = 0.8978, val_acc = 0.6000\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2290, train_acc = 0.9007, val_acc = 0.6250\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2292, train_acc = 0.9007, val_acc = 0.6450\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2174, train_acc = 0.9051, val_acc = 0.6200\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2177, train_acc = 0.9060, val_acc = 0.6300\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2215, train_acc = 0.9038, val_acc = 0.6250\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2269, train_acc = 0.8985, val_acc = 0.6450\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2218, train_acc = 0.9070, val_acc = 0.6200\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2428, train_acc = 0.9058, val_acc = 0.6300\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2399, train_acc = 0.9065, val_acc = 0.6150\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2157, train_acc = 0.9077, val_acc = 0.6000\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2290, train_acc = 0.9041, val_acc = 0.5950\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2157, train_acc = 0.9070, val_acc = 0.6350\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2210, train_acc = 0.9060, val_acc = 0.6300\n",
            "\n",
            "--------\n",
            "EPOCH 59\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5882\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2214, train_acc = 0.9007, val_acc = 0.6050\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2132, train_acc = 0.9058, val_acc = 0.5850\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2069, train_acc = 0.9092, val_acc = 0.6350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2368, train_acc = 0.9048, val_acc = 0.6250\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2275, train_acc = 0.9014, val_acc = 0.6200\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2093, train_acc = 0.9114, val_acc = 0.6100\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2218, train_acc = 0.9097, val_acc = 0.6300\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2284, train_acc = 0.9092, val_acc = 0.6250\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2247, train_acc = 0.9070, val_acc = 0.6150\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2294, train_acc = 0.9111, val_acc = 0.6250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2275, train_acc = 0.9104, val_acc = 0.6350\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2271, train_acc = 0.9041, val_acc = 0.6300\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2077, train_acc = 0.9075, val_acc = 0.6250\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2034, train_acc = 0.9085, val_acc = 0.6050\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2031, train_acc = 0.9065, val_acc = 0.6500\n",
            "\n",
            "--------\n",
            "EPOCH 60\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6670\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2212, train_acc = 0.9104, val_acc = 0.6450\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2067, train_acc = 0.9140, val_acc = 0.6500\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2060, train_acc = 0.9128, val_acc = 0.6350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2273, train_acc = 0.9068, val_acc = 0.6300\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2210, train_acc = 0.9034, val_acc = 0.6250\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2179, train_acc = 0.9060, val_acc = 0.6450\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2238, train_acc = 0.9072, val_acc = 0.6300\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2177, train_acc = 0.9041, val_acc = 0.6450\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2229, train_acc = 0.9053, val_acc = 0.6450\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2111, train_acc = 0.9075, val_acc = 0.6100\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2208, train_acc = 0.9077, val_acc = 0.6400\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2043, train_acc = 0.9114, val_acc = 0.6300\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2188, train_acc = 0.9089, val_acc = 0.6100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2303, train_acc = 0.9143, val_acc = 0.6200\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2176, train_acc = 0.9184, val_acc = 0.6150\n",
            "\n",
            "--------\n",
            "EPOCH 61\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3338\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2197, train_acc = 0.9089, val_acc = 0.6250\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2181, train_acc = 0.9089, val_acc = 0.6250\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2142, train_acc = 0.9087, val_acc = 0.6400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2124, train_acc = 0.9114, val_acc = 0.6050\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2007, train_acc = 0.9138, val_acc = 0.6050\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1914, train_acc = 0.9157, val_acc = 0.6150\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2046, train_acc = 0.9126, val_acc = 0.6100\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2038, train_acc = 0.9143, val_acc = 0.6300\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1993, train_acc = 0.9196, val_acc = 0.6350\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2036, train_acc = 0.9162, val_acc = 0.6400\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.1994, train_acc = 0.9182, val_acc = 0.6400\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1911, train_acc = 0.9201, val_acc = 0.6100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1929, train_acc = 0.9177, val_acc = 0.6450\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1923, train_acc = 0.9186, val_acc = 0.6450\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1951, train_acc = 0.9218, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 62\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5073\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.1993, train_acc = 0.9140, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.1875, train_acc = 0.9199, val_acc = 0.6000\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.1887, train_acc = 0.9184, val_acc = 0.5950\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2117, train_acc = 0.9109, val_acc = 0.6500\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.1974, train_acc = 0.9150, val_acc = 0.6450\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1930, train_acc = 0.9162, val_acc = 0.6250\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2185, train_acc = 0.9072, val_acc = 0.6450\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2076, train_acc = 0.9106, val_acc = 0.5850\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2124, train_acc = 0.9136, val_acc = 0.5950\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1915, train_acc = 0.9177, val_acc = 0.6000\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.1843, train_acc = 0.9155, val_acc = 0.6000\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1907, train_acc = 0.9165, val_acc = 0.6050\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2144, train_acc = 0.9085, val_acc = 0.6300\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2041, train_acc = 0.9121, val_acc = 0.5850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2038, train_acc = 0.9080, val_acc = 0.6100\n",
            "\n",
            "--------\n",
            "EPOCH 63\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3054\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2367, train_acc = 0.9089, val_acc = 0.5700\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2452, train_acc = 0.9087, val_acc = 0.5900\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2187, train_acc = 0.9104, val_acc = 0.6000\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2063, train_acc = 0.9094, val_acc = 0.5600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2034, train_acc = 0.9114, val_acc = 0.6000\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1993, train_acc = 0.9169, val_acc = 0.5900\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2050, train_acc = 0.9106, val_acc = 0.5750\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.1944, train_acc = 0.9121, val_acc = 0.5650\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2007, train_acc = 0.9136, val_acc = 0.5550\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1970, train_acc = 0.9153, val_acc = 0.5500\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.1874, train_acc = 0.9174, val_acc = 0.5900\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1902, train_acc = 0.9169, val_acc = 0.6000\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2027, train_acc = 0.9172, val_acc = 0.6150\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1969, train_acc = 0.9165, val_acc = 0.5850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1917, train_acc = 0.9148, val_acc = 0.5800\n",
            "\n",
            "--------\n",
            "EPOCH 64\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3972\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2568, train_acc = 0.9031, val_acc = 0.6100\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2177, train_acc = 0.9145, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2219, train_acc = 0.9063, val_acc = 0.6300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2080, train_acc = 0.9114, val_acc = 0.6000\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2052, train_acc = 0.9075, val_acc = 0.6150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2023, train_acc = 0.9111, val_acc = 0.6050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2298, train_acc = 0.9060, val_acc = 0.5800\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2125, train_acc = 0.9075, val_acc = 0.5800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2106, train_acc = 0.9077, val_acc = 0.6050\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2103, train_acc = 0.9087, val_acc = 0.5800\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2038, train_acc = 0.9140, val_acc = 0.5950\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1976, train_acc = 0.9128, val_acc = 0.6100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2105, train_acc = 0.9136, val_acc = 0.6450\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1958, train_acc = 0.9172, val_acc = 0.6300\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1895, train_acc = 0.9148, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 65\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.2565\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2231, train_acc = 0.9072, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2253, train_acc = 0.9048, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2179, train_acc = 0.9043, val_acc = 0.6250\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2168, train_acc = 0.9070, val_acc = 0.6600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2123, train_acc = 0.9114, val_acc = 0.6350\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2022, train_acc = 0.9114, val_acc = 0.6300\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2049, train_acc = 0.9174, val_acc = 0.6550\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2240, train_acc = 0.9145, val_acc = 0.6500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2110, train_acc = 0.9140, val_acc = 0.6750\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2192, train_acc = 0.9087, val_acc = 0.6450\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2064, train_acc = 0.9133, val_acc = 0.6400\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2008, train_acc = 0.9121, val_acc = 0.6350\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2168, train_acc = 0.9099, val_acc = 0.6300\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2131, train_acc = 0.9092, val_acc = 0.6150\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2028, train_acc = 0.9179, val_acc = 0.6150\n",
            "\n",
            "--------\n",
            "EPOCH 66\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3969\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2193, train_acc = 0.9080, val_acc = 0.6750\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2288, train_acc = 0.9065, val_acc = 0.6700\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2146, train_acc = 0.9072, val_acc = 0.6650\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2113, train_acc = 0.9119, val_acc = 0.6850\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2100, train_acc = 0.9099, val_acc = 0.6750\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2057, train_acc = 0.9126, val_acc = 0.6950\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2216, train_acc = 0.9053, val_acc = 0.6700\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2170, train_acc = 0.9070, val_acc = 0.6800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2108, train_acc = 0.9111, val_acc = 0.6850\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2122, train_acc = 0.9126, val_acc = 0.6900\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2103, train_acc = 0.9157, val_acc = 0.6750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2053, train_acc = 0.9143, val_acc = 0.6850\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2221, train_acc = 0.9191, val_acc = 0.6900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2020, train_acc = 0.9218, val_acc = 0.7100\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1959, train_acc = 0.9165, val_acc = 0.6550\n",
            "\n",
            "--------\n",
            "EPOCH 67\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.1273\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2105, train_acc = 0.9104, val_acc = 0.6950\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2016, train_acc = 0.9126, val_acc = 0.7150\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.1955, train_acc = 0.9131, val_acc = 0.7300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2249, train_acc = 0.9089, val_acc = 0.6550\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2186, train_acc = 0.9051, val_acc = 0.6550\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2135, train_acc = 0.9060, val_acc = 0.6700\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.1929, train_acc = 0.9111, val_acc = 0.6300\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.1916, train_acc = 0.9140, val_acc = 0.6650\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1892, train_acc = 0.9169, val_acc = 0.6500\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2109, train_acc = 0.9123, val_acc = 0.6600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2127, train_acc = 0.9138, val_acc = 0.7100\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2036, train_acc = 0.9150, val_acc = 0.7100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1914, train_acc = 0.9165, val_acc = 0.6900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1859, train_acc = 0.9216, val_acc = 0.6650\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1845, train_acc = 0.9182, val_acc = 0.6600\n",
            "\n",
            "--------\n",
            "EPOCH 68\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 9.9748\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2121, train_acc = 0.9133, val_acc = 0.5850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2090, train_acc = 0.9150, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.1989, train_acc = 0.9153, val_acc = 0.6200\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.1956, train_acc = 0.9189, val_acc = 0.6050\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.1985, train_acc = 0.9131, val_acc = 0.6150\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1962, train_acc = 0.9160, val_acc = 0.5900\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2576, train_acc = 0.9119, val_acc = 0.6000\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2437, train_acc = 0.9172, val_acc = 0.5800\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2455, train_acc = 0.9169, val_acc = 0.5800\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2262, train_acc = 0.9140, val_acc = 0.6300\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2069, train_acc = 0.9186, val_acc = 0.6350\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2023, train_acc = 0.9218, val_acc = 0.6050\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1988, train_acc = 0.9235, val_acc = 0.6100\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1903, train_acc = 0.9203, val_acc = 0.6300\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1855, train_acc = 0.9186, val_acc = 0.6350\n",
            "\n",
            "--------\n",
            "EPOCH 69\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.2636\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2013, train_acc = 0.9140, val_acc = 0.6250\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2078, train_acc = 0.9123, val_acc = 0.6300\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2165, train_acc = 0.9138, val_acc = 0.6050\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.1969, train_acc = 0.9111, val_acc = 0.6700\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2038, train_acc = 0.9111, val_acc = 0.6400\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.1917, train_acc = 0.9121, val_acc = 0.6450\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2002, train_acc = 0.9172, val_acc = 0.6350\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.1913, train_acc = 0.9191, val_acc = 0.6200\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.1878, train_acc = 0.9162, val_acc = 0.6450\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.1908, train_acc = 0.9191, val_acc = 0.6450\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.1935, train_acc = 0.9182, val_acc = 0.6550\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.1918, train_acc = 0.9206, val_acc = 0.6450\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.1930, train_acc = 0.9148, val_acc = 0.6400\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1953, train_acc = 0.9160, val_acc = 0.6500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1926, train_acc = 0.9094, val_acc = 0.6500\n",
            "\n",
            "--------\n",
            "EPOCH 70\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3112\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2166, train_acc = 0.9121, val_acc = 0.5650\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2218, train_acc = 0.9058, val_acc = 0.5400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2084, train_acc = 0.9143, val_acc = 0.5350\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2105, train_acc = 0.9116, val_acc = 0.5500\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2082, train_acc = 0.9109, val_acc = 0.5850\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2009, train_acc = 0.9068, val_acc = 0.5750\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2110, train_acc = 0.9131, val_acc = 0.5650\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2115, train_acc = 0.9128, val_acc = 0.6050\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2114, train_acc = 0.9116, val_acc = 0.5800\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2119, train_acc = 0.9138, val_acc = 0.5850\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2070, train_acc = 0.9085, val_acc = 0.5900\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2084, train_acc = 0.9133, val_acc = 0.5900\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2017, train_acc = 0.9150, val_acc = 0.5850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.1988, train_acc = 0.9128, val_acc = 0.6050\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.1960, train_acc = 0.9165, val_acc = 0.6100\n",
            "\n",
            "--------\n",
            "EPOCH 71\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.1150\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2274, train_acc = 0.8990, val_acc = 0.5600\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2242, train_acc = 0.9034, val_acc = 0.5350\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2253, train_acc = 0.8997, val_acc = 0.5750\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2111, train_acc = 0.9097, val_acc = 0.5500\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2227, train_acc = 0.9143, val_acc = 0.5600\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2069, train_acc = 0.9106, val_acc = 0.5300\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2334, train_acc = 0.9102, val_acc = 0.5100\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2057, train_acc = 0.9121, val_acc = 0.5500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2008, train_acc = 0.9119, val_acc = 0.5400\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2208, train_acc = 0.9065, val_acc = 0.5250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2286, train_acc = 0.9065, val_acc = 0.5250\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2112, train_acc = 0.9080, val_acc = 0.5300\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2272, train_acc = 0.9106, val_acc = 0.5300\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2196, train_acc = 0.9068, val_acc = 0.5500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2171, train_acc = 0.9087, val_acc = 0.5200\n",
            "\n",
            "--------\n",
            "EPOCH 72\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3524\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2293, train_acc = 0.8985, val_acc = 0.5900\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2275, train_acc = 0.9029, val_acc = 0.5950\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2148, train_acc = 0.9036, val_acc = 0.5550\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2274, train_acc = 0.9000, val_acc = 0.6050\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2126, train_acc = 0.9070, val_acc = 0.6050\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2139, train_acc = 0.9036, val_acc = 0.6050\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2153, train_acc = 0.8987, val_acc = 0.5850\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2119, train_acc = 0.9055, val_acc = 0.5600\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2079, train_acc = 0.9017, val_acc = 0.5850\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2148, train_acc = 0.9053, val_acc = 0.5900\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2097, train_acc = 0.9012, val_acc = 0.5700\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2096, train_acc = 0.9002, val_acc = 0.5900\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2183, train_acc = 0.9104, val_acc = 0.6000\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2090, train_acc = 0.9102, val_acc = 0.5950\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2063, train_acc = 0.9058, val_acc = 0.5950\n",
            "\n",
            "--------\n",
            "EPOCH 73\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.2166\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2207, train_acc = 0.9043, val_acc = 0.5750\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2294, train_acc = 0.9041, val_acc = 0.5400\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2191, train_acc = 0.9060, val_acc = 0.5600\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2498, train_acc = 0.8968, val_acc = 0.5300\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2393, train_acc = 0.9029, val_acc = 0.5600\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2242, train_acc = 0.9060, val_acc = 0.5550\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2613, train_acc = 0.9000, val_acc = 0.5500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2793, train_acc = 0.8987, val_acc = 0.5350\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2511, train_acc = 0.8949, val_acc = 0.5600\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2309, train_acc = 0.9051, val_acc = 0.5900\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2308, train_acc = 0.9046, val_acc = 0.5800\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2215, train_acc = 0.9048, val_acc = 0.5700\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2386, train_acc = 0.9065, val_acc = 0.5500\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2162, train_acc = 0.9102, val_acc = 0.5350\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2112, train_acc = 0.9082, val_acc = 0.5350\n",
            "\n",
            "--------\n",
            "EPOCH 74\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.3677\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2411, train_acc = 0.9063, val_acc = 0.5500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2267, train_acc = 0.9053, val_acc = 0.5600\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2259, train_acc = 0.9024, val_acc = 0.5800\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2351, train_acc = 0.8919, val_acc = 0.5850\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2337, train_acc = 0.9004, val_acc = 0.5700\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2184, train_acc = 0.9031, val_acc = 0.5500\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2175, train_acc = 0.9077, val_acc = 0.5600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2083, train_acc = 0.9063, val_acc = 0.5850\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2170, train_acc = 0.9097, val_acc = 0.5900\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2280, train_acc = 0.9077, val_acc = 0.5400\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2139, train_acc = 0.9092, val_acc = 0.5550\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2191, train_acc = 0.9089, val_acc = 0.6100\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2244, train_acc = 0.9070, val_acc = 0.5850\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2142, train_acc = 0.9085, val_acc = 0.5850\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2132, train_acc = 0.9111, val_acc = 0.5900\n",
            "\n",
            "--------\n",
            "EPOCH 75\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6284\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2410, train_acc = 0.9094, val_acc = 0.5850\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2256, train_acc = 0.9012, val_acc = 0.5800\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2252, train_acc = 0.9102, val_acc = 0.5400\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2230, train_acc = 0.8995, val_acc = 0.5650\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2191, train_acc = 0.9000, val_acc = 0.5800\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2066, train_acc = 0.9053, val_acc = 0.5750\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2301, train_acc = 0.9053, val_acc = 0.5600\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2338, train_acc = 0.9031, val_acc = 0.5650\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2323, train_acc = 0.9031, val_acc = 0.5650\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2292, train_acc = 0.9048, val_acc = 0.5850\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2181, train_acc = 0.9068, val_acc = 0.5900\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2014, train_acc = 0.9136, val_acc = 0.5950\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2105, train_acc = 0.9114, val_acc = 0.5550\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2093, train_acc = 0.9051, val_acc = 0.5500\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2163, train_acc = 0.9068, val_acc = 0.5350\n",
            "\n",
            "--------\n",
            "EPOCH 76\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.5921\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2349, train_acc = 0.9063, val_acc = 0.5500\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2326, train_acc = 0.9041, val_acc = 0.5600\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2235, train_acc = 0.9070, val_acc = 0.5550\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2056, train_acc = 0.9075, val_acc = 0.5800\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2010, train_acc = 0.9072, val_acc = 0.5900\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2061, train_acc = 0.9121, val_acc = 0.5650\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2078, train_acc = 0.9097, val_acc = 0.6000\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2087, train_acc = 0.9094, val_acc = 0.5950\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2093, train_acc = 0.9043, val_acc = 0.5800\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2187, train_acc = 0.9075, val_acc = 0.5600\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2116, train_acc = 0.9111, val_acc = 0.5750\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2134, train_acc = 0.9072, val_acc = 0.5600\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2177, train_acc = 0.9102, val_acc = 0.5900\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2084, train_acc = 0.9140, val_acc = 0.5750\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2130, train_acc = 0.9026, val_acc = 0.6150\n",
            "\n",
            "--------\n",
            "EPOCH 77\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.6028\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2308, train_acc = 0.9055, val_acc = 0.6300\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2192, train_acc = 0.9068, val_acc = 0.6200\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2145, train_acc = 0.9077, val_acc = 0.6300\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2126, train_acc = 0.9004, val_acc = 0.6600\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2044, train_acc = 0.9051, val_acc = 0.6750\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2085, train_acc = 0.9097, val_acc = 0.6400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2234, train_acc = 0.9070, val_acc = 0.6250\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2210, train_acc = 0.9055, val_acc = 0.6500\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2233, train_acc = 0.9060, val_acc = 0.6650\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2183, train_acc = 0.9106, val_acc = 0.6250\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2204, train_acc = 0.9094, val_acc = 0.6450\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2141, train_acc = 0.9087, val_acc = 0.6150\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2191, train_acc = 0.9104, val_acc = 0.6050\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2287, train_acc = 0.9068, val_acc = 0.6550\n",
            "d-step 5 epoch 3 : ......... average_loss = 0.2167, train_acc = 0.9104, val_acc = 0.6200\n",
            "\n",
            "--------\n",
            "EPOCH 78\n",
            "--------\n",
            "\n",
            "Adversarial Training Generator :  oracle_sample_NLL = 10.7629\n",
            "\n",
            "Adversarial Training Discriminator : \n",
            "d-step 1 epoch 1 : ......... average_loss = 0.2423, train_acc = 0.8924, val_acc = 0.6250\n",
            "d-step 1 epoch 2 : ......... average_loss = 0.2438, train_acc = 0.9007, val_acc = 0.6150\n",
            "d-step 1 epoch 3 : ......... average_loss = 0.2382, train_acc = 0.8997, val_acc = 0.6100\n",
            "d-step 2 epoch 1 : ......... average_loss = 0.2602, train_acc = 0.8992, val_acc = 0.5400\n",
            "d-step 2 epoch 2 : ......... average_loss = 0.2295, train_acc = 0.9043, val_acc = 0.5550\n",
            "d-step 2 epoch 3 : ......... average_loss = 0.2337, train_acc = 0.9017, val_acc = 0.5400\n",
            "d-step 3 epoch 1 : ......... average_loss = 0.2376, train_acc = 0.9070, val_acc = 0.5500\n",
            "d-step 3 epoch 2 : ......... average_loss = 0.2395, train_acc = 0.9089, val_acc = 0.5750\n",
            "d-step 3 epoch 3 : ......... average_loss = 0.2236, train_acc = 0.9089, val_acc = 0.5900\n",
            "d-step 4 epoch 1 : ......... average_loss = 0.2199, train_acc = 0.9109, val_acc = 0.5700\n",
            "d-step 4 epoch 2 : ......... average_loss = 0.2176, train_acc = 0.9053, val_acc = 0.5700\n",
            "d-step 4 epoch 3 : ......... average_loss = 0.2288, train_acc = 0.9080, val_acc = 0.5550\n",
            "d-step 5 epoch 1 : ......... average_loss = 0.2433, train_acc = 0.9014, val_acc = 0.5650\n",
            "d-step 5 epoch 2 : ......... average_loss = 0.2367, train_acc = 0.9024, val_acc = 0.5500\n",
            "d-step 5 epoch 3 : ...."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hU8zk3o2wbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "0ac1b21a-2436-4a52-fa47-7687da91e90c"
      },
      "source": [
        "#Sample sequences from the Generator\n",
        "\n",
        "#gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=False)\n",
        "#dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=False)\n",
        "\n",
        "#gen.cuda()\n",
        "#dis.cuda()\n",
        "\n",
        "#gen.load_state_dict(torch.load('/content/gen.pth',  map_location=torch.device('cpu')))\n",
        "#dis.load_state_dict(torch.load('/content/dis.pth',  map_location=torch.device('cpu')))\n",
        "\n",
        "#gen.eval()\n",
        "dis.eval()\n",
        "\n",
        "num_seqs = 25\n",
        "\n",
        "a = gen.sample(num_seqs).tolist()\n",
        "\n",
        "for i in range(num_seqs):\n",
        "  seq = (vector_to_sequence(a[i]))\n",
        "  seq = re.sub('[X]+$', '', seq)\n",
        "  check_x = re.search('[X]', seq)\n",
        "  if check_x:\n",
        "    print(seq + ', Invalid')\n",
        "  else:\n",
        "    print(\">pep\" + str(i))\n",
        "    print(seq)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">pep0\n",
            "ALGANLNAMVYGVYAACG\n",
            ">pep1\n",
            "AYTHDAGVGA\n",
            ">pep2\n",
            "RRKKALTGTAAAFFF\n",
            ">pep3\n",
            "SARRYPLRRAKRGGAR\n",
            ">pep4\n",
            "TPGA\n",
            ">pep5\n",
            "DVDAYDAVQITENKPLSD\n",
            ">pep6\n",
            "AFLLGTNQSLIF\n",
            ">pep7\n",
            "YWEDFFEFFKHFVRR\n",
            ">pep8\n",
            "KKKKVVAATYFFF\n",
            ">pep9\n",
            "DVTVQVTVCAVAALEALG\n",
            ">pep10\n",
            "WALNTRHYVH\n",
            ">pep11\n",
            "RNTVNLAELGKIGAIMLS\n",
            ">pep12\n",
            "GQWVV\n",
            ">pep13\n",
            "KKKKVLAFLYFSSA\n",
            ">pep14\n",
            "KDPAVLFPAAKLLKKAAG\n",
            ">pep15\n",
            "KKKKLVLPFLLF\n",
            ">pep16\n",
            "KKKKVLATLCF\n",
            ">pep17\n",
            "AGTPKFLRLK\n",
            ">pep18\n",
            "GLRLQGTAPNLGTYPKYY\n",
            ">pep19\n",
            "LPAAIDQRLDWIELNLEE\n",
            ">pep20\n",
            "QGTWRPGFLGFPFFFKVK\n",
            ">pep21\n",
            "ILMCCSVDP\n",
            ">pep22\n",
            "AESGDPRAVLLRVGYEVS\n",
            ">pep23\n",
            "KKKLVLSTYFFYSPYWWA\n",
            ">pep24\n",
            "ASGIGRNKFQKQIARGTM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vPKOBIsfHU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9dcb1028-163e-49d1-fd51-5b5d5b335e17"
      },
      "source": [
        "#Writes output of main() cell to file \n",
        "#which can be downloaded if needed\n",
        "\n",
        "with open('losses.txt', 'w') as f:\n",
        "    f.write(cap.stdout)\n",
        "\n",
        "Download = True\n",
        "\n",
        "if Download:\n",
        "  files.download('losses.txt') \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1d80cd6d-de36-4b5e-8bb4-de7291c8c4cc\", \"losses.txt\", 162764)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7kAuBfImGd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open('losses.txt', 'r') \n",
        "lines = file1.readlines() \n",
        "\n",
        "loss_g = []\n",
        "loss_d = []\n",
        "\n",
        "for line in lines:\n",
        "\n",
        "  reg_lossg = re.search('oracle_sample_NLL = ([-+]?[0-9]*\\.?[0-9]+)', line)\n",
        "  reg_lossd = re.search('average_loss = ([-+]?[0-9]*\\.?[0-9]+)', line)  \n",
        "  if reg_lossg:\n",
        "    loss_g.append(float(reg_lossg.group(1)))\n",
        "  if reg_lossd:\n",
        "    loss_d.append(float(reg_lossd.group(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHI3oUu755P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0992d908-f80b-4923-b1d4-6d1ac9994395"
      },
      "source": [
        "#Which loss to plot? 'g' = Generator, 'd' = Discriminator\n",
        "\n",
        "plt_loss = 'g'\n",
        "\n",
        "if plt_loss == 'g':\n",
        "  plt.title('Learning Curve  - Generator')\n",
        "  plt.ylabel('NLL by Oracle')\n",
        "  plt.xlabel('Update')\n",
        "  plt.plot(loss_g[100:])\n",
        "\n",
        "if plt_loss == 'd':\n",
        "  plt.title('Learning Curve - Discriminator')\n",
        "  plt.ylabel('BCE Loss')\n",
        "  plt.xlabel('Update')\n",
        "  plt.plot(loss_d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBV5pqLg_pna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the Models\n",
        "\n",
        "torch.save(gen.state_dict(), '/content/gen.pth')\n",
        "torch.save(dis.state_dict(), '/content/dis.pth')\n",
        "\n",
        "#Load the Models\n",
        "# model = Generator(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5LdNaWQcpiw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "beb0f947-a7b0-4fcf-8d2c-200d96aa76f6"
      },
      "source": [
        "files.download('dis.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5a58417c-985a-4262-bd89-f9eb21502699\", \"dis.pth\", 124417)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uka-EQ-ucvUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "8cf0c997-761d-4908-dd06-b189d5e0a61b"
      },
      "source": [
        "# gen = Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=True)\n",
        "# dis = Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=True)\n",
        "\n",
        "# gen.cuda()\n",
        "# dis.cuda()\n",
        "\n",
        "# gen.load_state_dict(torch.load('/content/gen.pth'))\n",
        "# dis.load_state_dict(torch.load('/content/dis.pth'))\n",
        "\n",
        "# gen.eval()\n",
        "# dis.eval()\n",
        "\n",
        "print('\\nStarting Adversarial Training...')\n",
        "oracle_loss = batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
        "                                            start_letter=START_LETTER, gpu=CUDA)\n",
        "print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
        "\n",
        "loss_frozen = []\n",
        "\n",
        "for epoch in range(100):\n",
        "    print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
        "    # TRAIN GENERATOR\n",
        "    print('\\nAdversarial Training Generator : ', end='')\n",
        "    sys.stdout.flush()\n",
        "    train_generator_PG(gen, gen_optimizer, oracle, dis, 1)\n",
        "\n",
        "    TRAIN DISCRIMINATOR\n",
        "   print('\\nSkkipping adversarial Training Discriminator : ')\n",
        "   train_discriminator(dis, dis_optimizer, oracle_samples, gen, oracle, ADV_D_STEPS, ADV_D_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (embeddings): Embedding(22, 3)\n",
              "  (gru): GRU(3, 32, num_layers=2, dropout=0.2, bidirectional=True)\n",
              "  (gru2hidden): Linear(in_features=128, out_features=32, bias=True)\n",
              "  (dropout_linear): Dropout(p=0.2, inplace=False)\n",
              "  (hidden2out): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x2DUI6YfyQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "69661976-f097-4d11-a518-eea8fa7a9c69"
      },
      "source": [
        "def noise():\n",
        "  print(\"AAAAAAAHHHH\")\n",
        "\n",
        "a = noise()\n",
        "\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AAAAAAAHHHH\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC1RU9biq6sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}