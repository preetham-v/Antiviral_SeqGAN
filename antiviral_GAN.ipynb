{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "antiviral_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMXnKfpWYi98hnehldju5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetham-v/antiviralGAN/blob/master/antiviral_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvVxFsCD9mT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwA9cE0NHuh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/preetham-v/antiviralGAN/master/AVPdb_data.csv'\n",
        "\n",
        "data = pd.read_csv(url, skiprows = 1, usecols = range(3), header=None, names=['ID','seq','len'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPnBUMVMJEK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76d79cb9-e536-4211-dee6-07aef231b241"
      },
      "source": [
        "all_sequences = np.asarray(data['seq'])\n",
        "print(all_sequences[:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PYVGSGLYRR' 'SMIENLEYM' 'ECRSTSYAGAVVNDL' 'STSYAGAVVNDL' 'YAGAVVNDL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQinJOXdHrU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dictionary of 20 canonical amino acids\n",
        "CHARACTER_DICT = set([\n",
        "    u'A', u'C', u'E', u'D', u'G', u'F', u'I', u'H', u'K',\n",
        "    u'M', u'L', u'N', u'Q', u'P', u'S', u'R', u'T', u'W',\n",
        "    u'V', u'Y']\n",
        ")\n",
        "\n",
        "CHARACTER_TO_INDEX = {\n",
        "    character: i\n",
        "    for i, character in enumerate(CHARACTER_DICT)\n",
        "}\n",
        "\n",
        "INDEX_TO_CHARACTER = {\n",
        "    CHARACTER_TO_INDEX[c]: c\n",
        "    for c in CHARACTER_TO_INDEX\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqm5jJ_NG1pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 18 #Problem Statement requires < 2000 kDa, Avg. amino acid = 110 kDa\n",
        "num_amino_acids = len(CHARACTER_DICT) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCHSqHfIOg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sequence_to_vector(sequence, embed_dict=None):\n",
        "    if embed_dict==None:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH, len(CHARACTER_TO_INDEX)+1])\n",
        "        default[:,len(CHARACTER_TO_INDEX)] = 1\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            default[i][CHARACTER_TO_INDEX[character]] = 1\n",
        "            default[i][len(CHARACTER_TO_INDEX)] = 0\n",
        "        return default\n",
        "    else:\n",
        "        default = np.zeros([MAX_SEQUENCE_LENGTH,len(embed_dict['A'])])\n",
        "        for i, character in enumerate(sequence[:MAX_SEQUENCE_LENGTH]):\n",
        "            for k,val in enumerate(embed_dict[character]):\n",
        "                default[i,k] = val\n",
        "        return default"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtK8LnPeImWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "303ef9a7-eab4-4034-9c73-241e36f64290"
      },
      "source": [
        "sequence = 'ECRSTSYAGAVVNDL'\n",
        "encoding = sequence_to_vector(sequence)\n",
        "#encoding = np.reshape(encoding, 378)\n",
        "print(encoding)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYPLrCaQnXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_length, output_activation=None):\n",
        "        \"\"\"A generator for mapping a random peptide to an antiviral peptide\n",
        "        Args:\n",
        "            input_length (int array): max_length * number_of_characters \n",
        "                                      (\"noise vector\")\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "            output_activation: torch activation function or None\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_length, 378)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.linear2 = nn.Linear(378, 378)\n",
        "        self.linear3 = nn.Linear(378, 378)\n",
        "        self.linear4 = nn.Linear(378, 378)\n",
        "        self.linear5 = nn.Linear(378, 378)\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        intermediate = self.linear1(input_tensor)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear2(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear3(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear4(intermediate)\n",
        "        intermediate = self.leaky_relu(intermediate)\n",
        "        intermediate = self.linear5(intermediate)\n",
        "        if self.output_activation is not None:\n",
        "            intermediate = self.output_activation(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN3i4Xoq9mqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, layers):\n",
        "        \"\"\"A discriminator for discerning real from generated samples.\n",
        "        params:\n",
        "            input_dim (int): width of the input\n",
        "            layers (List[int]): A list of layer widths including output width\n",
        "        Output activation is Sigmoid.\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self._init_layers(layers)\n",
        "\n",
        "    def _init_layers(self, layers):\n",
        "        \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
        "        self.module_list = nn.ModuleList()\n",
        "        last_layer = self.input_dim\n",
        "        for index, width in enumerate(layers):\n",
        "            self.module_list.append(nn.Linear(last_layer, width))\n",
        "            last_layer = width\n",
        "            if index + 1 != len(layers):\n",
        "                self.module_list.append(nn.Sigmoid())\n",
        "        else:\n",
        "            self.module_list.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
        "        intermediate = input_tensor\n",
        "        for layer in self.module_list:\n",
        "            intermediate = layer(intermediate)\n",
        "        return intermediate"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXgni6k90HhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_inputs = []\n",
        "for j in range(len(all_sequences)):\n",
        "  embedding = sequence_to_vector(all_sequences[j])\n",
        "  all_inputs.append(np.reshape(embedding,378))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIaAh98vmoNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_function(batch_size, iteration):\n",
        "  input_array = all_inputs[batch_size*iteration : batch_size*(iteration+1)]\n",
        "  return input_array\n",
        "\n",
        "def noise_function(batch_size):\n",
        "  input_array = []  \n",
        "  for j in range(batch_size):\n",
        "    a = np.zeros([18,21])\n",
        "    for i in range(18):\n",
        "      x = np.random.randint(21)\n",
        "      a[i][x] = 1\n",
        "    input_array.append(np.reshape(a,378))\n",
        "  \n",
        "  return input_array"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gphmNsOt-ikc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class VanillaGAN():\n",
        "    def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
        "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
        "        \"\"\"A GAN class for holding and training a generator and discriminator\n",
        "        Args:\n",
        "            generator: a Ganerator network\n",
        "            discriminator: A Discriminator network\n",
        "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
        "            data_fn: function f(num: int) -> pytorch tensor, (real samples)\n",
        "            batch_size: training batch size\n",
        "            device: cpu or CUDA\n",
        "            lr_d: learning rate for the discriminator\n",
        "            lr_g: learning rate for the generator\n",
        "        \"\"\"\n",
        "        self.generator = generator\n",
        "        self.generator = self.generator.to(device)\n",
        "        self.discriminator = discriminator\n",
        "        self.discriminator = self.discriminator.to(device)\n",
        "        self.noise_fn = noise_fn\n",
        "        self.data_fn = data_fn\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optim_d = optim.Adam(discriminator.parameters(),\n",
        "                                  lr=lr_d, betas=(0.5, 0.999))\n",
        "        self.optim_g = optim.Adam(generator.parameters(),\n",
        "                                  lr=lr_g, betas=(0.5, 0.999))\n",
        "        self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
        "        self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "    def generate_samples(self, latent_vec=None, num=None):\n",
        "        \"\"\"Sample from the generator.\n",
        "        Args:\n",
        "            latent_vec: A pytorch latent vector or None\n",
        "            num: The number of samples to generate if latent_vec is None\n",
        "        If latent_vec and num are None then us self.batch_size random latent\n",
        "        vectors.\n",
        "        \"\"\"\n",
        "        num = self.batch_size if num is None else num\n",
        "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
        "        with torch.no_grad():\n",
        "            samples = self.generator(latent_vec)\n",
        "        return samples\n",
        "\n",
        "    def train_step_generator(self):\n",
        "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
        "        self.generator.zero_grad()\n",
        "\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        generated = self.generator(latent_vec)\n",
        "        classifications = self.discriminator(generated)\n",
        "        loss = self.criterion(classifications, self.target_ones)\n",
        "        loss.backward()\n",
        "        self.optim_g.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def train_step_discriminator(self):\n",
        "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
        "        self.discriminator.zero_grad()\n",
        "\n",
        "        # real samples\n",
        "        real_samples = self.data_fn(self.batch_size)\n",
        "        pred_real = self.discriminator(real_samples)\n",
        "        loss_real = self.criterion(pred_real, self.target_ones)\n",
        "\n",
        "        # generated samples\n",
        "        latent_vec = self.noise_fn(self.batch_size)\n",
        "        with torch.no_grad():\n",
        "            fake_samples = self.generator(latent_vec)\n",
        "        pred_fake = self.discriminator(fake_samples)\n",
        "        loss_fake = self.criterion(pred_fake, self.target_zeros)\n",
        "\n",
        "        # combine\n",
        "        loss = (loss_real + loss_fake) / 2\n",
        "        loss.backward()\n",
        "        self.optim_d.step()\n",
        "        return loss_real.item(), loss_fake.item()\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"Train both networks and return the losses.\"\"\"\n",
        "        loss_d = self.train_step_discriminator()\n",
        "        loss_g = self.train_step_generator()\n",
        "        return loss_g, loss_d"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC7m2z-iXxGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simpleGAN(batch_size: int = 25, epochs: int = 5, max_data: int = len(all_sequences), print_every: int = 10):\n",
        "\n",
        "  #Array to monitor losses\n",
        "  loss_g = []\n",
        "  loss_d = []\n",
        "  input_length = 378\n",
        "\n",
        "  # Models\n",
        "  generator = Generator(input_length)\n",
        "  discriminator = Discriminator(input_length, [180, 45, 1])  \n",
        "\n",
        "  # Optimizers\n",
        "  generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "  discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), \n",
        "                                          lr=0.001)\n",
        "\n",
        "  # loss\n",
        "  loss = nn.BCELoss()\n",
        "\n",
        "  for i in range(epochs):\n",
        "\n",
        "    for j in range(int(max_data/batch_size)):\n",
        "\n",
        "      # zero the gradients on each iteration\n",
        "      generator_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "      # Create noisy input for generator\n",
        "      # Need float type instead of int\n",
        "      noise = noise_function(batch_size)\n",
        "      noise_data = torch.tensor(noise).float()\n",
        "      generated_data = generator(noise_data)  \n",
        "\n",
        "      # Generate examples of real data\n",
        "      true_data = data_function(batch_size, j)\n",
        "      true_labels = torch.tensor(np.ones(batch_size)).float()\n",
        "      true_data = torch.tensor(true_data).float()\n",
        "\n",
        "      # Train the generator\n",
        "      # We invert the labels here and don't train the discriminator because we want the generator\n",
        "      # to make things the discriminator classifies as true.\n",
        "      generator_discriminator_out = discriminator(generated_data)\n",
        "      generator_loss = loss(generator_discriminator_out, true_labels)\n",
        "      generator_loss.backward()\n",
        "      generator_optimizer.step()\n",
        "\n",
        "      # Train the discriminator on the true/generated data\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      true_discriminator_out = discriminator(true_data)\n",
        "      true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
        "\n",
        "      # add .detach() here think about this\n",
        "      generator_discriminator_out = discriminator(generated_data.detach())\n",
        "      generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
        "      discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2   \n",
        "      discriminator_loss.backward()\n",
        "      discriminator_optimizer.step()\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    if i % print_every == 0:\n",
        "      generated_data = generated_data.detach().numpy()\n",
        "#      x = random.randint(0,len(generated_data))\n",
        "      sequence = np.reshape(generated_data[0], [18,21])\n",
        "      sequence = (sequence == sequence.max(axis=1)[:,None]).astype(int)\n",
        "\n",
        "      print(i, sequence)\n",
        "      print(generator_loss, discriminator_loss)    \n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jUxoWZeFFy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "def main():\n",
        "    from time import time\n",
        "    epochs = 600\n",
        "    batches = 10\n",
        "    generator = Generator(360)\n",
        "    discriminator = Discriminator(1, [64, 32, 1])\n",
        "    noise_fn = noise_function()\n",
        "    data_fn = data_function()\n",
        "    gan = VanillaGAN(generator, discriminator, noise_fn, data_fn, device='cpu')\n",
        "    loss_g, loss_d_real, loss_d_fake = [], [], []\n",
        "    start = time()\n",
        "    for epoch in range(epochs):\n",
        "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
        "        for batch in range(batches):\n",
        "            lg_, (ldr_, ldf_) = gan.train_step()\n",
        "            loss_g_running += lg_\n",
        "            loss_d_real_running += ldr_\n",
        "            loss_d_fake_running += ldf_\n",
        "        loss_g.append(loss_g_running / batches)\n",
        "        loss_d_real.append(loss_d_real_running / batches)\n",
        "        loss_d_fake.append(loss_d_fake_running / batches)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
        "              f\" G={loss_g[-1]:.3f},\"\n",
        "              f\" Dr={loss_d_real[-1]:.3f},\"\n",
        "              f\" Df={loss_d_fake[-1]:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v5ReVWzaZeJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "936efef1-7e4d-4741-a2a1-280437681f8b"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "simpleGAN(batch_size=100, epochs=1001, print_every = 100)\n",
        "print(\"Program took\", time.time() - start_time, \"to run\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "tensor(0.8642, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.5911, grad_fn=<DivBackward0>)\n",
            "100 [[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "tensor(5.8489, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.0034, grad_fn=<DivBackward0>)\n",
            "200 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "tensor(7.6309, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.0006, grad_fn=<DivBackward0>)\n",
            "300 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(9.0335, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.0002, grad_fn=<DivBackward0>)\n",
            "400 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(9.7345, grad_fn=<BinaryCrossEntropyBackward>) tensor(6.8616e-05, grad_fn=<DivBackward0>)\n",
            "500 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(11.1472, grad_fn=<BinaryCrossEntropyBackward>) tensor(1.7208e-05, grad_fn=<DivBackward0>)\n",
            "600 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(12.4019, grad_fn=<BinaryCrossEntropyBackward>) tensor(5.9396e-06, grad_fn=<DivBackward0>)\n",
            "700 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(13.3471, grad_fn=<BinaryCrossEntropyBackward>) tensor(2.1082e-06, grad_fn=<DivBackward0>)\n",
            "800 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(11.5089, grad_fn=<BinaryCrossEntropyBackward>) tensor(5.6595e-06, grad_fn=<DivBackward0>)\n",
            "900 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(13.8867, grad_fn=<BinaryCrossEntropyBackward>) tensor(1.1843e-06, grad_fn=<DivBackward0>)\n",
            "1000 [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
            "tensor(20.0287, grad_fn=<BinaryCrossEntropyBackward>) tensor(5.9605e-08, grad_fn=<DivBackward0>)\n",
            "Program took 746.7926955223083 to run\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drm_MN96pPUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}